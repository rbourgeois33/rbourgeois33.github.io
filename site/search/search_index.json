{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Click on the posts !","title":"Home"},{"location":"about/","text":"Nothing to see here...","title":"About"},{"location":"posts/post1/","text":"Six basic performance advices for porting kernels to the GPU. 0. Introduction Some context and motivations Hello world ! This is my first blog post. I'm R\u00e9mi Bourgeois, PhD. I am a researcher engineer working at the French Atomic Energy Comission (CEA). I work on the TRUST platform , a HPC (multi-GPU) CFD code that serves as a basis for many research/industrial applications. I was hired by CEA to join the porting effort of this legacy code to the GPU using Kokkos . This is quite a challenging task as the code is 20 years old, and more than 1400 kernels were identified to be ported to the GPU ! As I went and optimized some kernels, something struck me: The nature of the task of porting code to the GPU, especially when time is limited, often lead to small mistakes that can undermine performance. The goal of this blogpost is to give you basic , easy tips to keep in mind when writing / porting / first optimizing your kernels, so that you get a reasonable performance. By applying them, I was able to get the following speedups that are measured relative to an already GPU-enabled baseline: A 40-50% speedup on a CFD convection kernel from TRUST (obtained on RTX A5000, RTX A6000 Ada and H100 GPUs). Brace yourself : this is a monstruous kernel. A 20-50% speedup on a CFD diffusion kernel from TRUST (obtained on RTX A6000 Ada and H100 GPUs). A 20% speedup on a MUSCL reconstruction kernel from the radiative hydrodynamics code heraclescpp (obtained on a A100 GPU) TODO: add ncu reports By reasonable I do not mean that you will get optimal performance. In fact, I will not go over what I consider to be advanced optimization advices such as the use of shared memory , vectorized operations , tensor cores operations , hardware-specific optimizations . If getting optimal performance is crucial to your application, consider learning more and apply these, but keep in mind that performance often comes at the cost of portability . The advices are general enough so that they should allow speedups on all cards from all vendors. By advanced , I do not mean that these topics are especially difficult or out of reach, but only that they require a significant design effort to be used effectively in a production context such as a wide CFD code like TRUST. In contrast, I believe that the advices I will give to you in this blogpost are easy enough so that you can, and should apply them straightforwardly while porting your code to the GPU in a time limited environement. Note 1: The target audience is engineer / researcher that want to get started with GPU porting in a code that relies on custom, domain specific low-level kernel. But do not reinvent the wheel ! i.e. do not rewrite kernels that have been implemented, hihgly optimized and distributed in libraries. Consider looking into (non exhaustive list !): CUDA Libraries . kokkos kernels for portable BLAS, sparse BLAS and fraph kernels. Trilinos for high level, portable solutions for the solution of large-scale, complex multi-physics engineering and scientific problems. PETSc for the scalable solution of scientific applications modeled by partial differential equations (PDEs) Disclaimers If you think I wrote something that is wrong, or misleading please let me know ! I am running my performance tests on Nvidia GPUs, just because they are more easily available to me, and that I am more familiar with the performance tools such as nsight systems (nsys) and nsight compute (ncu). However, note that AMD provides similar profilers, and that the advices that I give here are simple enought so that they apply for GPUs from both vendors. Moreover, I will use Kokkos as the programming model, just because I work with it, and that performance portability is cool . Again, the concepts are simple enought so that you can translate them to your favorite programming model, OpenMP, SYCL, Cuda, Hip. Pre-requisits In this small tutorial, I will assume that you are already familiar with / will not cover: Basic C++. The reason why you might want to use the GPU, and that you need a big enough problem to make full use of it. How to Compile a GPU code, generate a report with Nvidia nsight compute and loading in with the ui. What is Kokkos, why you might want to use it and how to get started with it. Some ressources: Talk by Christian Trott, Co-leader of the Kokkos core team . Kokkos lecture series (kind of outdated, but you can find a lot of ressources online, alos, join the slack !). Note: you really should consider using Kokkos, or any other portable programming model. It's good enough so that CEA adopted it for it's legacy codes ! (see the CExA project ). Basic GPU architecture, in particular: That you should avoid host to device memory transfers. The roofline performance model. What does compute bound / memory bound mean. Some knowledge about the memory hierarchy (registers, L1/L2 caches, DRAM) and the increasing cost of memory accesses. Some ressources: 1h30 lecture from Athena Elfarou on GPU architecture / CUDA programming 13 lectures on CUDA programming by Bob Crovella Application-level optimization: How to build a sensible optimization roadmap with e.g. Nvidia Nsight System How to ensure that it is worth it to optimize the kernel you are looking (Don't assume bottleneck, profile, assess, optimize). Some ressouces: 8th lecture from the Bob Crovella lecture series which focuses on that topic. Outline The outline for this post is the following 6 rules of thumbs,or advices, largely inspired by the Nvidia Ampere tuning guide : Minimise redundant global memory accesses. Ensure memory access are coalesced. Minimize redundant math operation, use cheap arithmetics. Understanding occupancy. Avoid the use of Local memory . Avoid thread divergence. For each topic, I will provide: An explanation of why you need to worry about it, \"Background\" subsections, How to detect that it is limiting your kernel performance using ncu , \"Profiler diagnosis\" subsections, How to fix / avoid the issue, \"Advices\" subsections. Feel free to jump straight into your sections of interest. Before we start Before going into the 6 advices, I invite you to read my post on the cost of communications that is a good, unesseray long introduction for advices 1. and 2. I also strongly advise watching this brilliant talk on communication-avoiding algorithms . 1. Minimise redundant global memory accesses Background Profiler diagnosis Advices use register variable + static array, might need to template 2. Ensure memory access are coalesced Background Profiler diagnosis Advices Think about your data Layout Kokkos layout conspiracy The granularity of memory accesses: lost bytes 3. Minimize redundant math operation, use cheap arithmetics Background Profiler diagnosis Advices FMA, / vs *, unroll loop for int computation might need to template Math can be a bottleneck Do smarter math 4. Understanding occupancy Background Profiler diagnosis Advices Hide latency The occupancy trap, ILP, hide latency reduce Register usage Reduce usage, launch bound, no MDRANGE, we dont talk about block and share memory limits, template away expensive branches 5. Avoid the use of Local memory Background Profiler diagnosis Advices Local memory is SLOW Why does it spills + ref a la precedente section How to avoid stack usage attention aux tableaux statiques 6. Avoid thread divergence Background Profiler diagnosis Advices The SIMD pattern, masking templating Final advices Participate to hackathons !","title":"Six basic performance advices for porting kernels to the GPU"},{"location":"posts/post1/#six-basic-performance-advices-for-porting-kernels-to-the-gpu","text":"","title":"Six basic performance advices for porting kernels to the GPU."},{"location":"posts/post1/#0-introduction","text":"","title":"0. Introduction"},{"location":"posts/post1/#some-context-and-motivations","text":"Hello world ! This is my first blog post. I'm R\u00e9mi Bourgeois, PhD. I am a researcher engineer working at the French Atomic Energy Comission (CEA). I work on the TRUST platform , a HPC (multi-GPU) CFD code that serves as a basis for many research/industrial applications. I was hired by CEA to join the porting effort of this legacy code to the GPU using Kokkos . This is quite a challenging task as the code is 20 years old, and more than 1400 kernels were identified to be ported to the GPU ! As I went and optimized some kernels, something struck me: The nature of the task of porting code to the GPU, especially when time is limited, often lead to small mistakes that can undermine performance. The goal of this blogpost is to give you basic , easy tips to keep in mind when writing / porting / first optimizing your kernels, so that you get a reasonable performance. By applying them, I was able to get the following speedups that are measured relative to an already GPU-enabled baseline: A 40-50% speedup on a CFD convection kernel from TRUST (obtained on RTX A5000, RTX A6000 Ada and H100 GPUs). Brace yourself : this is a monstruous kernel. A 20-50% speedup on a CFD diffusion kernel from TRUST (obtained on RTX A6000 Ada and H100 GPUs). A 20% speedup on a MUSCL reconstruction kernel from the radiative hydrodynamics code heraclescpp (obtained on a A100 GPU) TODO: add ncu reports By reasonable I do not mean that you will get optimal performance. In fact, I will not go over what I consider to be advanced optimization advices such as the use of shared memory , vectorized operations , tensor cores operations , hardware-specific optimizations . If getting optimal performance is crucial to your application, consider learning more and apply these, but keep in mind that performance often comes at the cost of portability . The advices are general enough so that they should allow speedups on all cards from all vendors. By advanced , I do not mean that these topics are especially difficult or out of reach, but only that they require a significant design effort to be used effectively in a production context such as a wide CFD code like TRUST. In contrast, I believe that the advices I will give to you in this blogpost are easy enough so that you can, and should apply them straightforwardly while porting your code to the GPU in a time limited environement. Note 1: The target audience is engineer / researcher that want to get started with GPU porting in a code that relies on custom, domain specific low-level kernel. But do not reinvent the wheel ! i.e. do not rewrite kernels that have been implemented, hihgly optimized and distributed in libraries. Consider looking into (non exhaustive list !): CUDA Libraries . kokkos kernels for portable BLAS, sparse BLAS and fraph kernels. Trilinos for high level, portable solutions for the solution of large-scale, complex multi-physics engineering and scientific problems. PETSc for the scalable solution of scientific applications modeled by partial differential equations (PDEs)","title":"Some context and motivations"},{"location":"posts/post1/#disclaimers","text":"If you think I wrote something that is wrong, or misleading please let me know ! I am running my performance tests on Nvidia GPUs, just because they are more easily available to me, and that I am more familiar with the performance tools such as nsight systems (nsys) and nsight compute (ncu). However, note that AMD provides similar profilers, and that the advices that I give here are simple enought so that they apply for GPUs from both vendors. Moreover, I will use Kokkos as the programming model, just because I work with it, and that performance portability is cool . Again, the concepts are simple enought so that you can translate them to your favorite programming model, OpenMP, SYCL, Cuda, Hip.","title":"Disclaimers"},{"location":"posts/post1/#pre-requisits","text":"In this small tutorial, I will assume that you are already familiar with / will not cover: Basic C++. The reason why you might want to use the GPU, and that you need a big enough problem to make full use of it. How to Compile a GPU code, generate a report with Nvidia nsight compute and loading in with the ui. What is Kokkos, why you might want to use it and how to get started with it. Some ressources: Talk by Christian Trott, Co-leader of the Kokkos core team . Kokkos lecture series (kind of outdated, but you can find a lot of ressources online, alos, join the slack !). Note: you really should consider using Kokkos, or any other portable programming model. It's good enough so that CEA adopted it for it's legacy codes ! (see the CExA project ). Basic GPU architecture, in particular: That you should avoid host to device memory transfers. The roofline performance model. What does compute bound / memory bound mean. Some knowledge about the memory hierarchy (registers, L1/L2 caches, DRAM) and the increasing cost of memory accesses. Some ressources: 1h30 lecture from Athena Elfarou on GPU architecture / CUDA programming 13 lectures on CUDA programming by Bob Crovella Application-level optimization: How to build a sensible optimization roadmap with e.g. Nvidia Nsight System How to ensure that it is worth it to optimize the kernel you are looking (Don't assume bottleneck, profile, assess, optimize). Some ressouces: 8th lecture from the Bob Crovella lecture series which focuses on that topic.","title":"Pre-requisits"},{"location":"posts/post1/#outline","text":"The outline for this post is the following 6 rules of thumbs,or advices, largely inspired by the Nvidia Ampere tuning guide : Minimise redundant global memory accesses. Ensure memory access are coalesced. Minimize redundant math operation, use cheap arithmetics. Understanding occupancy. Avoid the use of Local memory . Avoid thread divergence. For each topic, I will provide: An explanation of why you need to worry about it, \"Background\" subsections, How to detect that it is limiting your kernel performance using ncu , \"Profiler diagnosis\" subsections, How to fix / avoid the issue, \"Advices\" subsections. Feel free to jump straight into your sections of interest.","title":"Outline"},{"location":"posts/post1/#before-we-start","text":"Before going into the 6 advices, I invite you to read my post on the cost of communications that is a good, unesseray long introduction for advices 1. and 2. I also strongly advise watching this brilliant talk on communication-avoiding algorithms .","title":"Before we start"},{"location":"posts/post1/#1-minimise-redundant-global-memory-accesses","text":"","title":"1. Minimise redundant global memory accesses"},{"location":"posts/post1/#background","text":"","title":"Background"},{"location":"posts/post1/#profiler-diagnosis","text":"","title":"Profiler diagnosis"},{"location":"posts/post1/#advices","text":"use register variable + static array, might need to template","title":"Advices"},{"location":"posts/post1/#2-ensure-memory-access-are-coalesced","text":"","title":"2. Ensure memory access are coalesced"},{"location":"posts/post1/#background_1","text":"","title":"Background"},{"location":"posts/post1/#profiler-diagnosis_1","text":"","title":"Profiler diagnosis"},{"location":"posts/post1/#advices_1","text":"Think about your data Layout Kokkos layout conspiracy The granularity of memory accesses: lost bytes","title":"Advices"},{"location":"posts/post1/#3-minimize-redundant-math-operation-use-cheap-arithmetics","text":"","title":"3. Minimize redundant math operation, use cheap arithmetics"},{"location":"posts/post1/#background_2","text":"","title":"Background"},{"location":"posts/post1/#profiler-diagnosis_2","text":"","title":"Profiler diagnosis"},{"location":"posts/post1/#advices_2","text":"FMA, / vs *, unroll loop for int computation might need to template Math can be a bottleneck Do smarter math","title":"Advices"},{"location":"posts/post1/#4-understanding-occupancy","text":"","title":"4. Understanding occupancy"},{"location":"posts/post1/#background_3","text":"","title":"Background"},{"location":"posts/post1/#profiler-diagnosis_3","text":"","title":"Profiler diagnosis"},{"location":"posts/post1/#advices_3","text":"Hide latency The occupancy trap, ILP, hide latency reduce Register usage Reduce usage, launch bound, no MDRANGE, we dont talk about block and share memory limits, template away expensive branches","title":"Advices"},{"location":"posts/post1/#5-avoid-the-use-of-local-memory","text":"","title":"5. Avoid the use of Local memory"},{"location":"posts/post1/#background_4","text":"","title":"Background"},{"location":"posts/post1/#profiler-diagnosis_4","text":"","title":"Profiler diagnosis"},{"location":"posts/post1/#advices_4","text":"Local memory is SLOW Why does it spills + ref a la precedente section How to avoid stack usage attention aux tableaux statiques","title":"Advices"},{"location":"posts/post1/#6-avoid-thread-divergence","text":"","title":"6. Avoid thread divergence"},{"location":"posts/post1/#background_5","text":"","title":"Background"},{"location":"posts/post1/#profiler-diagnosis_5","text":"","title":"Profiler diagnosis"},{"location":"posts/post1/#advices_5","text":"The SIMD pattern, masking templating","title":"Advices"},{"location":"posts/post1/#final-advices","text":"Participate to hackathons !","title":"Final advices"},{"location":"posts/post2/","text":"The cost of memory transfers Disclaimer This post was orginally supposed to be a short introduction within my first post on GPU kernel optimisation , but then I realized that I liked to talk too much about it. This is largely inspired by this brilliant talk by Prof. James Demmel (Berkley), as well as a the his CS267 class with free lectures on Youtube, where you can find everything that I explain here. Note The terms communications and memory transfers will be used interchangeably. Also in the present context, a word refers to a single FP64 number. Hardware trends Looking at Figure 1, memory transfers, either within DRAM, or over the network, have been more expensive than (FP64) math operation since ~1992: Figure 1: Evolution of the time per flop (gamma), inverse bandwitdh (beta) and latency (alpha) between ~1980 to ~2015. Source . The graph stops around 2015, where the ratio of gamma to beta (DRAM) was around 10. Let's look at the current FP64 Flop Per Load (FPL) factor for more recent hardware: GPU Release Year FP64 FLOPS (TFLOPS) BW (TB/s) FPL V100 2017 7.066 0.897 ~65.19 A100 2020 9.746 1.56 ~49.9 H100 2022 25.61 2.04 ~ 100 B200 2024 62 8.20 ~60 Table 1: Evolution of the BW, FP64 flops and FPL for recent Nvidia GPU models. FPL is computed as \\(\\frac{FP64 \\ Flops}{BW}*8\\) since a FP64 number is made of 8 bytes. Note It is illuminating to think about FPL as the average amount of FP64 operation you can do in the time that it takes to load one word from DRAM. As we can see, the FPL has been oscillating between 50 and 100. This is really large, and should motivate you to think really hard about how you access memory. In fact, communication avoiding algorithms have been a very active and fruitful research area which lead to the development of the BLAS standard, the Lapack library, and much more. A simple memory model One result that I like a lot is the one presented in the second CS267 lecture where the following simple memory model is proposed: Assume a simple machine with just 2 levels of memory, fast and slow (think of e.g. DRAM / registers) and the following properties and notations: \\(M=\\) number of words that fits into fast memory, No latency (simplifying assumption), \\(t_m=\\) time per slow memory operation e.g. to moove a word from fast to slow memory (inverse BW from Table 1 multiplied by 8 in our case since we are doing FP64 and ignoring latency), \\(t_f=\\) time per artithmetic operation i.e. the inverse of the FLOPS in Table 1. Assume an implementation of an algorithm with: \\(m=\\) number of words moved bewteen fast and slow memory to complete the algorithm, \\(f=\\) number of arithmetic operations to complete the algorithm, We can then define \\(CI_{\\text{implem}}=\\frac{f}{m}\\) , a property of the implementation of the algorithm that is called the computational intensity .It is the average number of flops per slow memory access. While the previously defined FPL factor, a property of the machine , is just given as \\(FPL_{\\text{hardware}}=\\frac{t_m}{t_f}\\) . Note: Nvidia GPUs have 4 levels of memory: DRAM, L2 and L1 caches, and registers. Each level has ~an order of magnitude difference in bandwithd. CPUs have 5 levels with an additional L3 cache. Real memory models are super complicated ! However, it is clear that memory hierachies are omnipresent. As a result, the advices presented here also help CPU performance ! Getting good performance The minimum possible time for the our algorithm is \\(t_{\\text{ideal}}=f*t_f\\) , which is attained when the problem fits in fast memory ( \\(m<M\\) ) and no slow memory transaction are required. This implies that we don't read any intial data from slow memory nor store in it, this is never the case in practice. Let's compare this to the real time for a big enough problem \\(t_{\\text{real}}=f*t_f+m*t_m\\) which rewrites: \\(t_{\\text{real}}= t_{\\text{ideal}}(1+\\frac{FPL_{\\text{hardware}}}{CI_{\\text{implem}}})\\) It is now clear that to get near optimal performance, we want to reduce the ratio \\(\\frac{FPL_{\\text{hardware}}}{CI_{\\text{implem}}}\\) as much as possible. Since \\({FPL_{\\text{hardware}}}\\) is a property of the hardware, with a value ranging between 50 and 100 depending on the GPU considered, all we can do is try to reduce \\(\\frac{1}{CI_{\\text{implem}}}=\\frac{m}{f}\\) , by trying to reuse the data we load as much as possible. Properties of an algorithm vs. properties of an implementation I insist on using the terminology implementation of an algorithm because in practice, the numbers \\(f\\) , \\(m\\) and \\(CI_{\\text{implem}}=\\frac{f}{m}\\) should not be obtained by simply computing the ratio of how much memory should be touched, and how many operation should be done ideally, optimally for a given algorithm. Because most real problems do not fit in cache. Instead, these numbers are a property of how the algorithm is implemented, compiled and ran. In fact, they can vary dramatically between a naive and a smart implementation. Let's consider the example of a very generic algorithm: dense matrix multiplication , \\(C=A.B\\) (the 1st homework of CS267 and topic of the 2nd and 3rd lectures). If three \\(n\\times n\\) matrices fits in fast memory, we know that that we need to load/store only \\(3n^2\\) words (2 matrix read, 1 matrix write) from slow memory, and perform \\(2n^3\\) operations (one dot product per element of C, each dot product being \\(n\\) multiply and \\(n\\) add) with a resulting \\(CI_{\\text{ideal}}^{\\text{matmul}}=\\frac{3}{2n}\\) . The bigger \\(n\\) is, the closer we get from ideal performance. However, as \\(n\\) grows, it is clear that the problem does not fit in fast memory ( \\(3n^2>M\\) eventually). Then, a naive implementation of matrix multiply such as: for i in range(n): #load A[i,:] in fast memory for j in range (n): #load B[:,j] in fast memory C[i,j] = dot(A[i,:], B[:,j]) #store C[i,:] in fast memory can be shown to have a computational intensity \\(CI_{\\text{naive}}^{\\text{matmul}}=\\mathcal{O}(1)\\) , which is terrible ! On the other hand, the well-known blocked implementation that splits and iterates over \\(b\\times b\\) sub-blocks of the matrices has a computational intensity of \\(CI_{\\text{blocked}}^{\\text{matmul}}=\\mathcal{O}(b)\\) , assuming that the blocks fit in fast memory. So, you might wonder, what should I do ? How do I know if there is a better algorithm ? Well, a theoretical upper bound on the computational intensity has been found and is given by \\(CI_{\\text{blocked}}^{\\text{matmul}}=\\mathcal{O}(\\sqrt{M})\\) , and if you ever write a new dense matmul implementation, you should strive to reach it. And notice ! the blocked algortihm reaches that bound. Indeed, since the blocks fit in fast memory, \\(3b^2 <=M\\) \\(\\implies\\) \\(b=\\mathcal{O}(\\sqrt{M})\\) . This is the whole point of communication avoiding algorithms research: computing lower bounds and finding algorithm that reaches them. Again, if you find this interesting, consider looking at this brilliant introduction . Conclusion Well, all that is quite fascinating, but also overwhelming don't you think ? Well, you might not have to think about all this lower bound theory to get good speedups. In my first post on GPU kernel optimisation I go over frequent coding mistakes that leads to extra useless communications. In that posr, I don't want to even try to pretend that I can help you computing nor reaching the theoretical communication lower bound for your algorithm. This is just too general to be discussed in a blogpost, and as we saw, it constitutes a resarch topic on it's own and implies a deep re-thinking of the algorithms and data structures. No, here we will stay simple and focus on the following: given a GPU kernel, what frequent coding mistakes should we avoid to limit the amount of data we load/store from slow memory.","title":"The cost of communications"},{"location":"posts/post2/#the-cost-of-memory-transfers","text":"","title":"The cost of memory transfers"},{"location":"posts/post2/#disclaimer","text":"This post was orginally supposed to be a short introduction within my first post on GPU kernel optimisation , but then I realized that I liked to talk too much about it. This is largely inspired by this brilliant talk by Prof. James Demmel (Berkley), as well as a the his CS267 class with free lectures on Youtube, where you can find everything that I explain here. Note The terms communications and memory transfers will be used interchangeably. Also in the present context, a word refers to a single FP64 number.","title":"Disclaimer"},{"location":"posts/post2/#hardware-trends","text":"Looking at Figure 1, memory transfers, either within DRAM, or over the network, have been more expensive than (FP64) math operation since ~1992: Figure 1: Evolution of the time per flop (gamma), inverse bandwitdh (beta) and latency (alpha) between ~1980 to ~2015. Source . The graph stops around 2015, where the ratio of gamma to beta (DRAM) was around 10. Let's look at the current FP64 Flop Per Load (FPL) factor for more recent hardware: GPU Release Year FP64 FLOPS (TFLOPS) BW (TB/s) FPL V100 2017 7.066 0.897 ~65.19 A100 2020 9.746 1.56 ~49.9 H100 2022 25.61 2.04 ~ 100 B200 2024 62 8.20 ~60 Table 1: Evolution of the BW, FP64 flops and FPL for recent Nvidia GPU models. FPL is computed as \\(\\frac{FP64 \\ Flops}{BW}*8\\) since a FP64 number is made of 8 bytes. Note It is illuminating to think about FPL as the average amount of FP64 operation you can do in the time that it takes to load one word from DRAM. As we can see, the FPL has been oscillating between 50 and 100. This is really large, and should motivate you to think really hard about how you access memory. In fact, communication avoiding algorithms have been a very active and fruitful research area which lead to the development of the BLAS standard, the Lapack library, and much more.","title":"Hardware trends"},{"location":"posts/post2/#a-simple-memory-model","text":"One result that I like a lot is the one presented in the second CS267 lecture where the following simple memory model is proposed: Assume a simple machine with just 2 levels of memory, fast and slow (think of e.g. DRAM / registers) and the following properties and notations: \\(M=\\) number of words that fits into fast memory, No latency (simplifying assumption), \\(t_m=\\) time per slow memory operation e.g. to moove a word from fast to slow memory (inverse BW from Table 1 multiplied by 8 in our case since we are doing FP64 and ignoring latency), \\(t_f=\\) time per artithmetic operation i.e. the inverse of the FLOPS in Table 1. Assume an implementation of an algorithm with: \\(m=\\) number of words moved bewteen fast and slow memory to complete the algorithm, \\(f=\\) number of arithmetic operations to complete the algorithm, We can then define \\(CI_{\\text{implem}}=\\frac{f}{m}\\) , a property of the implementation of the algorithm that is called the computational intensity .It is the average number of flops per slow memory access. While the previously defined FPL factor, a property of the machine , is just given as \\(FPL_{\\text{hardware}}=\\frac{t_m}{t_f}\\) . Note: Nvidia GPUs have 4 levels of memory: DRAM, L2 and L1 caches, and registers. Each level has ~an order of magnitude difference in bandwithd. CPUs have 5 levels with an additional L3 cache. Real memory models are super complicated ! However, it is clear that memory hierachies are omnipresent. As a result, the advices presented here also help CPU performance !","title":"A simple memory model"},{"location":"posts/post2/#getting-good-performance","text":"The minimum possible time for the our algorithm is \\(t_{\\text{ideal}}=f*t_f\\) , which is attained when the problem fits in fast memory ( \\(m<M\\) ) and no slow memory transaction are required. This implies that we don't read any intial data from slow memory nor store in it, this is never the case in practice. Let's compare this to the real time for a big enough problem \\(t_{\\text{real}}=f*t_f+m*t_m\\) which rewrites: \\(t_{\\text{real}}= t_{\\text{ideal}}(1+\\frac{FPL_{\\text{hardware}}}{CI_{\\text{implem}}})\\) It is now clear that to get near optimal performance, we want to reduce the ratio \\(\\frac{FPL_{\\text{hardware}}}{CI_{\\text{implem}}}\\) as much as possible. Since \\({FPL_{\\text{hardware}}}\\) is a property of the hardware, with a value ranging between 50 and 100 depending on the GPU considered, all we can do is try to reduce \\(\\frac{1}{CI_{\\text{implem}}}=\\frac{m}{f}\\) , by trying to reuse the data we load as much as possible.","title":"Getting good performance"},{"location":"posts/post2/#properties-of-an-algorithm-vs-properties-of-an-implementation","text":"I insist on using the terminology implementation of an algorithm because in practice, the numbers \\(f\\) , \\(m\\) and \\(CI_{\\text{implem}}=\\frac{f}{m}\\) should not be obtained by simply computing the ratio of how much memory should be touched, and how many operation should be done ideally, optimally for a given algorithm. Because most real problems do not fit in cache. Instead, these numbers are a property of how the algorithm is implemented, compiled and ran. In fact, they can vary dramatically between a naive and a smart implementation. Let's consider the example of a very generic algorithm: dense matrix multiplication , \\(C=A.B\\) (the 1st homework of CS267 and topic of the 2nd and 3rd lectures). If three \\(n\\times n\\) matrices fits in fast memory, we know that that we need to load/store only \\(3n^2\\) words (2 matrix read, 1 matrix write) from slow memory, and perform \\(2n^3\\) operations (one dot product per element of C, each dot product being \\(n\\) multiply and \\(n\\) add) with a resulting \\(CI_{\\text{ideal}}^{\\text{matmul}}=\\frac{3}{2n}\\) . The bigger \\(n\\) is, the closer we get from ideal performance. However, as \\(n\\) grows, it is clear that the problem does not fit in fast memory ( \\(3n^2>M\\) eventually). Then, a naive implementation of matrix multiply such as: for i in range(n): #load A[i,:] in fast memory for j in range (n): #load B[:,j] in fast memory C[i,j] = dot(A[i,:], B[:,j]) #store C[i,:] in fast memory can be shown to have a computational intensity \\(CI_{\\text{naive}}^{\\text{matmul}}=\\mathcal{O}(1)\\) , which is terrible ! On the other hand, the well-known blocked implementation that splits and iterates over \\(b\\times b\\) sub-blocks of the matrices has a computational intensity of \\(CI_{\\text{blocked}}^{\\text{matmul}}=\\mathcal{O}(b)\\) , assuming that the blocks fit in fast memory. So, you might wonder, what should I do ? How do I know if there is a better algorithm ? Well, a theoretical upper bound on the computational intensity has been found and is given by \\(CI_{\\text{blocked}}^{\\text{matmul}}=\\mathcal{O}(\\sqrt{M})\\) , and if you ever write a new dense matmul implementation, you should strive to reach it. And notice ! the blocked algortihm reaches that bound. Indeed, since the blocks fit in fast memory, \\(3b^2 <=M\\) \\(\\implies\\) \\(b=\\mathcal{O}(\\sqrt{M})\\) . This is the whole point of communication avoiding algorithms research: computing lower bounds and finding algorithm that reaches them. Again, if you find this interesting, consider looking at this brilliant introduction .","title":"Properties of an algorithm vs. properties of an implementation"},{"location":"posts/post2/#conclusion","text":"Well, all that is quite fascinating, but also overwhelming don't you think ? Well, you might not have to think about all this lower bound theory to get good speedups. In my first post on GPU kernel optimisation I go over frequent coding mistakes that leads to extra useless communications. In that posr, I don't want to even try to pretend that I can help you computing nor reaching the theoretical communication lower bound for your algorithm. This is just too general to be discussed in a blogpost, and as we saw, it constitutes a resarch topic on it's own and implies a deep re-thinking of the algorithms and data structures. No, here we will stay simple and focus on the following: given a GPU kernel, what frequent coding mistakes should we avoid to limit the amount of data we load/store from slow memory.","title":"Conclusion"}]}
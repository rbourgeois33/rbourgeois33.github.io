{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My Blog Go to my first article Go to my second article","title":"Home"},{"location":"#welcome-to-my-blog","text":"Go to my first article Go to my second article","title":"Welcome to My Blog"},{"location":"about/","text":"","title":"About"},{"location":"posts/post1/","text":"Basic performance tricks for porting kernels to the GPU. Some context and motivations Hello world ! This is my first blog post. I'm R\u00e9mi Bourgeois, PhD. I am a researcher engineer working at the French Atomic Energy Comission (CEA). I work on the TRUST platform , a HPC (multi-GPU) CFD code that serves as a basis for many research/industrial applications. I was hired by CEA to join the porting effort of this legacy code to the GPU using Kokkos . This is quite a challenging task as the code is 20 years old, and more than 1400 kernels were identified to be ported to the GPU ! As I went and optimized some kernels, something struck me: The nature of the task of porting code to the GPU, especially when time is limited, often lead to small mistakes that can undermine performance. The goal of this blogpost is to give you basic , easy tips to keep in mind when writing / porting / first optimizing your kernels, so that you get a reasonable performance. By applying them, I was able to get the following speedups that are measured relative to an already GPU-enabled baseline: A 40-50% speedup on a CFD convection kernel from TRUST (obtained on RTX A5000, RTX A6000 Ada and H100 GPUs). Brace yourself : this is a monstruous kernel. A 20-50% speedup on a CFD diffusion kernel from TRUST (obtained on RTX A6000 Ada and H100 GPUs). A 20% speedup on a MUSCL reconstruction kernel from the radiative hydrodynamics code heraclescpp (obtained on a A100 GPU) TODO: add ncu reports By reasonable I do not mean that you will get optimal performance. In fact, I will not go over what I consider to be advanced optimization tricks such as the use of shared memory , vectorized operations , tensor cores operations , hardware-specific optimizations . If getting optimal performance is crucial to your application, consider learning more and apply these, but keep in mind that performance often comes at the cost of portability . The advices are general enough so that they should allow speedups on all cards from all vendors. By advanced , I do not mean that these topics are especially difficult or out of reach, but only that they require a significant design effort to be used effectively in a production context such as a wide CFD code like TRUST. In contrast, I believe that the tricks I will give to you in this blogpost are easy enough so that you can, and should apply them straightforwardly while porting your code to the GPU in a time limited environement. Note 1: The target audience is engineer / researcher that want to get started with GPU porting in a code that relies on custom, domain specific low-level kernel. But do not reinvent the wheel ! i.e. do not rewrite kernels that have been implemented, hihgly optimized and distributed in libraries. Consider looking into (non exhaustive list !): CUDA Libraries . kokkos kernels for portable BLAS, sparse BLAS and fraph kernels. Trilinos for high level, portable solutions for the solution of large-scale, complex multi-physics engineering and scientific problems. PETSc for the scalable solution of scientific applications modeled by partial differential equations (PDEs) Note 2: If you don't care about the why's and want to jumpt straight into the How's, go straight into them with the side menu ! Disclaimer & Requirements Disclaimers If you think I wrote something that is wrong, or misleading please let me know ! I am running my performance tests on Nvidia GPUs, just because they are more easily available to me, and that I am more familiar with the performance tools such as nsight systems (nsys) and nsight compute (ncu). However, note that AMD provides similar profilers, and that the advices that I give here are simple enought so that they apply for GPUs from both vendors. Moreover, I will use Kokkos as the programming model, just because I work with it, and that performance portability is cool . Again, the concepts are simple enought so that you can translate them to your favorite programming model, OpenMP, SYCL, Cuda, Hip. Requirements In this small tutorial, I will assume that you are already familiar with / will not cover: Basic C++. The reason why you might want to use the GPU, and that you need a big enough problem to make full use of it. How to Compile a GPU code, generate a report with Nvidia nsight compute and loading in with the ui. What is Kokkos, why you might want to use it and how to get started with it. Some ressources: Talk by Christian Trott, Co-leader of the Kokkos core team . Kokkos lecture series (kind of outdated, but you can find a lot of ressources online, alos, join the slack !). Note: you really should consider using Kokkos, or any other portable programming model. It's good enough so that CEA adopted it for it's legacy codes ! (see the CExA project ). Basic GPU architecture, in particular: That you should avoid host to device memory transfers. The roofline performance model. What does compute bound / memory bound mean. Some knowledge about the memory hierarchy (registers, L1/L2 caches, DRAM) and the increasing cost of memory accesses. Some ressources: 1h30 lecture from Athena Elfarou on GPU architecture / CUDA programming 13 lectures on CUDA programming by Bob Crovella Application-level optimization: How to build a sensible optimization roadmap with e.g. Nvidia Nsight System How to ensure that it is worth it to optimize the kernel you are looking (Don't assume bottleneck, profile, assess, optimize). Some ressouces: 8th lecture from the Bob Crovella lecture series which focuses on that topic. Outline The outline for this post is the following: Minimise redundant global memory accesses. Ensure memory access are coalesced. Minimize redundant math operation, use cheap arithmetics. Understanding occupancy. Avoid the use of Local memory . Avoid thread divergence. For each topic, I will provide: - An explanation of why you need to worry about it, - How to detect that it is limiting your kernel performance using ncu, - How to fix the issue Minimise redundant global memory accesses Why: the cost of memory transfers Looking at Figure 1, memory transfers, either within DRAM, or over the network, has been more expensive than (FP64) math operation since ~1992: Figure 1: Evolution of the time per flop (gamma), inverse bandwitdh (beta) and latency (alpha) between ~1980 to ~2015. Source . The graph stops around 2015, where the ratio of time per flop to bandwith time-scale was around 10. Let's look at the ratio of Flops to Bandwith for recent Nvidia GPUs: GPU Release Year FP64 FLOPS (TFLOPS) BW (TB/s) FLOPs per Double V100 2017 7.066 0.897 ~65.19 A100 2020 9.746 1.56 ~49.9 H100 2022 25.61 2.04 ~ 100 B200 2024 62 8.20 ~60 Table 1: Evolution of the BW, FP64 flops and Flops per double for recent Nvidia GPU models. Last row is computed as $\\frac{FP64 \\ Flops}{BW}*8(bytes)$. I think it is illuminating to look at the value in the last row. It represents the average amount of FP64 operation you can do in the time that it takes to load one FP64 word. As we can see, it has been oscillating between 50 and 100. This should motivate you to think really hard about how you access memory. In fact, \"communication avoiding algorithms\" has been a very active and fruitful research area which lead to the development of the BLAS standard, the Lapack library, and much more. Here is a brilliant introductory lecture on that topic by Prof. James Demmel (Berkley), as well as a the webpage his CS267 class with free lectures on Youtube. It is so important in fact that researchers have been computing theoretical bounds for the computational intensity of crucial algorithm, and worked hard to design such bound attaining programs. The bottom line is, you should make good use of the word you read from slow memory (DRAM) into fast memory (caches, registers), and avoid redundant read/writes. What to look for in ncu How: use register variables static array, might need to template Ensure memory access are coalesced What to look for in ncu Why: The granularity of memory accesses: lost bytes How: Think about your data Layout Kokkos layout conspiracy Minimize redundant math operation, use cheap arithmetics What to look for in ncu Why: Math can be a bottleneck How: Do smarter math FMA, / vs *, unroll loop for int computation might need to template Understanding occupancy What to look for in ncu Why: Hide latency The occupancy trap, ILP, hide latency How: Reduce Register usage Reduce usage, launch bound, no MDRANGE, we dont talk about block and share memory limits, template away expensive branches Avoid the use of Local memory What to look for in ncu / compile output Why: Local memory is SLOW How How to avoid register spilling Why does it spills + ref a la precedente section How to avoid stack usage attention aux tableaux statiques Avoid thread divergence What to look for in ncu Why: The SIMD pattern, masking How: templating Final advices Participate to hackathons !","title":"First Post"},{"location":"posts/post1/#basic-performance-tricks-for-porting-kernels-to-the-gpu","text":"","title":"Basic performance tricks for porting kernels to the GPU."},{"location":"posts/post1/#some-context-and-motivations","text":"Hello world ! This is my first blog post. I'm R\u00e9mi Bourgeois, PhD. I am a researcher engineer working at the French Atomic Energy Comission (CEA). I work on the TRUST platform , a HPC (multi-GPU) CFD code that serves as a basis for many research/industrial applications. I was hired by CEA to join the porting effort of this legacy code to the GPU using Kokkos . This is quite a challenging task as the code is 20 years old, and more than 1400 kernels were identified to be ported to the GPU ! As I went and optimized some kernels, something struck me: The nature of the task of porting code to the GPU, especially when time is limited, often lead to small mistakes that can undermine performance. The goal of this blogpost is to give you basic , easy tips to keep in mind when writing / porting / first optimizing your kernels, so that you get a reasonable performance. By applying them, I was able to get the following speedups that are measured relative to an already GPU-enabled baseline: A 40-50% speedup on a CFD convection kernel from TRUST (obtained on RTX A5000, RTX A6000 Ada and H100 GPUs). Brace yourself : this is a monstruous kernel. A 20-50% speedup on a CFD diffusion kernel from TRUST (obtained on RTX A6000 Ada and H100 GPUs). A 20% speedup on a MUSCL reconstruction kernel from the radiative hydrodynamics code heraclescpp (obtained on a A100 GPU) TODO: add ncu reports By reasonable I do not mean that you will get optimal performance. In fact, I will not go over what I consider to be advanced optimization tricks such as the use of shared memory , vectorized operations , tensor cores operations , hardware-specific optimizations . If getting optimal performance is crucial to your application, consider learning more and apply these, but keep in mind that performance often comes at the cost of portability . The advices are general enough so that they should allow speedups on all cards from all vendors. By advanced , I do not mean that these topics are especially difficult or out of reach, but only that they require a significant design effort to be used effectively in a production context such as a wide CFD code like TRUST. In contrast, I believe that the tricks I will give to you in this blogpost are easy enough so that you can, and should apply them straightforwardly while porting your code to the GPU in a time limited environement. Note 1: The target audience is engineer / researcher that want to get started with GPU porting in a code that relies on custom, domain specific low-level kernel. But do not reinvent the wheel ! i.e. do not rewrite kernels that have been implemented, hihgly optimized and distributed in libraries. Consider looking into (non exhaustive list !): CUDA Libraries . kokkos kernels for portable BLAS, sparse BLAS and fraph kernels. Trilinos for high level, portable solutions for the solution of large-scale, complex multi-physics engineering and scientific problems. PETSc for the scalable solution of scientific applications modeled by partial differential equations (PDEs) Note 2: If you don't care about the why's and want to jumpt straight into the How's, go straight into them with the side menu !","title":"Some context and motivations"},{"location":"posts/post1/#disclaimer-requirements","text":"","title":"Disclaimer &amp; Requirements"},{"location":"posts/post1/#disclaimers","text":"If you think I wrote something that is wrong, or misleading please let me know ! I am running my performance tests on Nvidia GPUs, just because they are more easily available to me, and that I am more familiar with the performance tools such as nsight systems (nsys) and nsight compute (ncu). However, note that AMD provides similar profilers, and that the advices that I give here are simple enought so that they apply for GPUs from both vendors. Moreover, I will use Kokkos as the programming model, just because I work with it, and that performance portability is cool . Again, the concepts are simple enought so that you can translate them to your favorite programming model, OpenMP, SYCL, Cuda, Hip.","title":"Disclaimers"},{"location":"posts/post1/#requirements","text":"In this small tutorial, I will assume that you are already familiar with / will not cover: Basic C++. The reason why you might want to use the GPU, and that you need a big enough problem to make full use of it. How to Compile a GPU code, generate a report with Nvidia nsight compute and loading in with the ui. What is Kokkos, why you might want to use it and how to get started with it. Some ressources: Talk by Christian Trott, Co-leader of the Kokkos core team . Kokkos lecture series (kind of outdated, but you can find a lot of ressources online, alos, join the slack !). Note: you really should consider using Kokkos, or any other portable programming model. It's good enough so that CEA adopted it for it's legacy codes ! (see the CExA project ). Basic GPU architecture, in particular: That you should avoid host to device memory transfers. The roofline performance model. What does compute bound / memory bound mean. Some knowledge about the memory hierarchy (registers, L1/L2 caches, DRAM) and the increasing cost of memory accesses. Some ressources: 1h30 lecture from Athena Elfarou on GPU architecture / CUDA programming 13 lectures on CUDA programming by Bob Crovella Application-level optimization: How to build a sensible optimization roadmap with e.g. Nvidia Nsight System How to ensure that it is worth it to optimize the kernel you are looking (Don't assume bottleneck, profile, assess, optimize). Some ressouces: 8th lecture from the Bob Crovella lecture series which focuses on that topic.","title":"Requirements"},{"location":"posts/post1/#outline","text":"The outline for this post is the following: Minimise redundant global memory accesses. Ensure memory access are coalesced. Minimize redundant math operation, use cheap arithmetics. Understanding occupancy. Avoid the use of Local memory . Avoid thread divergence. For each topic, I will provide: - An explanation of why you need to worry about it, - How to detect that it is limiting your kernel performance using ncu, - How to fix the issue","title":"Outline"},{"location":"posts/post1/#minimise-redundant-global-memory-accesses","text":"","title":"Minimise redundant global memory accesses"},{"location":"posts/post1/#why-the-cost-of-memory-transfers","text":"Looking at Figure 1, memory transfers, either within DRAM, or over the network, has been more expensive than (FP64) math operation since ~1992: Figure 1: Evolution of the time per flop (gamma), inverse bandwitdh (beta) and latency (alpha) between ~1980 to ~2015. Source . The graph stops around 2015, where the ratio of time per flop to bandwith time-scale was around 10. Let's look at the ratio of Flops to Bandwith for recent Nvidia GPUs: GPU Release Year FP64 FLOPS (TFLOPS) BW (TB/s) FLOPs per Double V100 2017 7.066 0.897 ~65.19 A100 2020 9.746 1.56 ~49.9 H100 2022 25.61 2.04 ~ 100 B200 2024 62 8.20 ~60 Table 1: Evolution of the BW, FP64 flops and Flops per double for recent Nvidia GPU models. Last row is computed as $\\frac{FP64 \\ Flops}{BW}*8(bytes)$. I think it is illuminating to look at the value in the last row. It represents the average amount of FP64 operation you can do in the time that it takes to load one FP64 word. As we can see, it has been oscillating between 50 and 100. This should motivate you to think really hard about how you access memory. In fact, \"communication avoiding algorithms\" has been a very active and fruitful research area which lead to the development of the BLAS standard, the Lapack library, and much more. Here is a brilliant introductory lecture on that topic by Prof. James Demmel (Berkley), as well as a the webpage his CS267 class with free lectures on Youtube. It is so important in fact that researchers have been computing theoretical bounds for the computational intensity of crucial algorithm, and worked hard to design such bound attaining programs. The bottom line is, you should make good use of the word you read from slow memory (DRAM) into fast memory (caches, registers), and avoid redundant read/writes.","title":"Why: the cost of memory transfers"},{"location":"posts/post1/#what-to-look-for-in-ncu","text":"","title":"What to look for in ncu"},{"location":"posts/post1/#how-use-register-variables","text":"static array, might need to template","title":"How: use register variables"},{"location":"posts/post1/#ensure-memory-access-are-coalesced","text":"","title":"Ensure memory access are coalesced"},{"location":"posts/post1/#what-to-look-for-in-ncu_1","text":"","title":"What to look for in ncu"},{"location":"posts/post1/#why-the-granularity-of-memory-accesses-lost-bytes","text":"","title":"Why: The granularity of memory accesses: lost bytes"},{"location":"posts/post1/#how-think-about-your-data-layout","text":"Kokkos layout conspiracy","title":"How: Think about your data Layout"},{"location":"posts/post1/#minimize-redundant-math-operation-use-cheap-arithmetics","text":"","title":"Minimize redundant math operation, use cheap arithmetics"},{"location":"posts/post1/#what-to-look-for-in-ncu_2","text":"","title":"What to look for in ncu"},{"location":"posts/post1/#why-math-can-be-a-bottleneck","text":"","title":"Why: Math can be a bottleneck"},{"location":"posts/post1/#how-do-smarter-math","text":"FMA, / vs *, unroll loop for int computation might need to template","title":"How: Do smarter math"},{"location":"posts/post1/#understanding-occupancy","text":"","title":"Understanding occupancy"},{"location":"posts/post1/#what-to-look-for-in-ncu_3","text":"","title":"What to look for in ncu"},{"location":"posts/post1/#why-hide-latency","text":"The occupancy trap, ILP, hide latency","title":"Why: Hide latency"},{"location":"posts/post1/#how-reduce-register-usage","text":"Reduce usage, launch bound, no MDRANGE, we dont talk about block and share memory limits, template away expensive branches","title":"How: Reduce Register usage"},{"location":"posts/post1/#avoid-the-use-of-local-memory","text":"","title":"Avoid the use of Local memory"},{"location":"posts/post1/#what-to-look-for-in-ncu-compile-output","text":"","title":"What to look for in ncu / compile output"},{"location":"posts/post1/#why-local-memory-is-slow","text":"","title":"Why: Local memory is SLOW"},{"location":"posts/post1/#how","text":"","title":"How"},{"location":"posts/post1/#how-to-avoid-register-spilling","text":"Why does it spills + ref a la precedente section","title":"How to avoid register spilling"},{"location":"posts/post1/#how-to-avoid-stack-usage","text":"attention aux tableaux statiques","title":"How to avoid stack usage"},{"location":"posts/post1/#avoid-thread-divergence","text":"","title":"Avoid thread divergence"},{"location":"posts/post1/#what-to-look-for-in-ncu_4","text":"","title":"What to look for in ncu"},{"location":"posts/post1/#why-the-simd-pattern-masking","text":"","title":"Why: The SIMD pattern, masking"},{"location":"posts/post1/#how-templating","text":"","title":"How: templating"},{"location":"posts/post1/#final-advices","text":"Participate to hackathons !","title":"Final advices"},{"location":"posts/post2/","text":"","title":"Second Post"}]}
{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Click on the posts ! (WIP) Six basic performance advices for porting kernels to the GPU The cost of communications (WIP) Testing mixed precision at runtime with verrou (WIP) Scheduling different jobs on the same runner with Gitlab's CI","title":"Home"},{"location":"about/","text":"Hello world ! This is my first blog. I'm R\u00e9mi Bourgeois, PhD. I am a researcher engineer working at the French Atomic Energy Commission (CEA). I work on the TRUST platform , a HPC (multi-GPU) CFD code that serves as a basis for many research/industrial applications. My research / engineering interests include: GPU kernel / application optimization. Using / picking GPU-enabled linear algebra libraries wisely. Finite volume methods, MHD convection see my PhD thesis on the subject ! Continuous integration (gitlab / github). Communication avoiding algorithms. experimenting with reduced precision. The opinions expressed here are only mine, and do not reflect the opinions of CEA. Moreover, I am very prone to errors and quick conclusion. If you spot one, feel free to reach and discuss ! Links github , Linkedin , Email: remi.bourgeois33@gmail.com.","title":"About"},{"location":"about/#links","text":"github , Linkedin , Email: remi.bourgeois33@gmail.com.","title":"Links"},{"location":"posts/post1/","text":"(WIP) Six basic performance advices for porting kernels to the GPU. 0. Introduction Some context and motivations I was hired by CEA to join the porting effort of this legacy code to the GPU using Kokkos . This is quite a challenging task as the code is 20 years old, and more than 1400 kernels were identified to be ported to the GPU ! As I went and optimized some kernels, something struck me: The nature of the task of porting code to the GPU, especially when time is limited, often lead to small mistakes that can undermine performance. The goal of this blogpost is to give you basic , easy tips to keep in mind when writing / porting / first optimizing your kernels, so that you get a reasonable performance. By applying them, I was able to get the following speedups that are measured relative to an already GPU-enabled baseline: A 40-50% speedup on a CFD convection kernel from TRUST (obtained on RTX A5000, RTX A6000 Ada and H100 GPUs). Brace yourself : this is a monstruous kernel. A 20-50% speedup on a CFD diffusion kernel from TRUST (obtained on RTX A6000 Ada and H100 GPUs). A 20% speedup on a MUSCL reconstruction kernel from the radiative hydrodynamics code heraclescpp (obtained on a A100 GPU) By reasonable I do not mean that you will get optimal performance. In fact, I will not go over what I consider to be advanced optimization advices such as the use of shared memory , vectorized operations , tensor cores operations , hardware-specific optimizations . If getting optimal performance is crucial to your application, consider learning more and apply these, but keep in mind that performance often comes at the cost of portability . The advices are general enough so that they should allow speedups on all cards from all vendors. By advanced , I do not mean that these topics are especially difficult or out of reach, but only that they require a significant design effort to be used effectively in a production context such as a wide CFD code like TRUST. In contrast, I believe that the advices I will give to you in this blogpost are easy enough so that you can, and should apply them straightforwardly while porting your code to the GPU in a time limited environement. Note 1: The target audience is engineer / researcher that want to get started with GPU porting in a code that relies on custom, domain specific low-level kernel. But do not reinvent the wheel ! i.e. do not rewrite kernels that have been implemented, hihgly optimized and distributed in libraries. Consider looking into (non exhaustive list !): CUDA Libraries . kokkos kernels for portable BLAS, sparse BLAS and fraph kernels. Trilinos for high level, portable solutions for the solution of large-scale, complex multi-physics engineering and scientific problems. PETSc for the scalable solution of scientific applications modeled by partial differential equations (PDEs) Disclaimers If you think I wrote something that is wrong, or misleading please let me know ! I am running my performance tests on Nvidia GPUs, just because they are more easily available to me, and that I am more familiar with the performance tools such as nsight systems (nsys) and nsight compute (ncu). However, note that AMD provides similar profilers (altough, at the time I am writing this, rocm's kernel profilers seem a lot less user friendly), and that the advices that I give here are general enought so that they apply for GPUs from both vendors. Moreover, I will use Kokkos as the programming model for the code sample, just because I work with it, and that performance portability is cool . Again, the concepts are simple enought so that you can translate them to your favorite programming model, OpenMP, SYCL, Cuda, Hip. Pre-requisits In this small tutorial, I will assume that you are already familiar with: Basic C++. The reason why you might want to use the GPU, and that you need a big enough problem to make full use of it. Some understanding of GPU performance: The roofline performance model. What does compute bound / memory bound mean. Basic GPU architecture, in particular: Some knowledge about the memory hierarchy (registers, L1/L2 caches, DRAM) and the increasing cost of memory accesses. What are CUDA threads / blocks, global memory. You can be confused about what is local memory . Some ressources on GPU architecture / CUDA programming: 1h30 lecture by Athena Elfarou 13 lectures by Bob Crovella Altough not necessary for getting through this post, I recommend you learn about: How to compile a GPU code, generate a report with Nvidia nsight compute and loading in with the ui. Application-level optimization: How to build a sensible optimization roadmap with e.g. Nvidia Nsight System That you should avoid host to device memory transfers; This tutorial is centered on kernel-level optimisation. We assume memory is already available on the GPU How to ensure that it is worth it to optimize the kernel you are looking (Don't assume bottleneck, profile, assess, optimize). Some ressouces: 8th lecture from the Bob Crovella lecture series which focuses on that topic. What is Kokkos, why you might want to use it and how to get started with it. Some ressources: Talk by Christian Trott, Co-leader of the Kokkos core team . Kokkos lecture series (kind of outdated, but you can find a lot of ressources online, alos, join the slack !). Note: you really should consider using Kokkos, or any other portable programming model. It's good enough so that CEA adopted it for it's legacy codes ! (see the CExA project ). Outline The outline for this post is the following six rules of thumbs,or advices, largely inspired by the Nvidia Ampere tuning guide : Minimize redundant global memory accesses. Minimize redundant math operation, use cheap arithmetics. Understanding occupancy. Avoid the use of Local memory . Avoid thread divergence. Feel free to jump straight into your sections of interest. One of the main interest of this tutorial is to teach you where to look for information in ncu. Look for \"Profiler diagnosis\" sections. Before we start Before going into the 6 advices, I invite you to read my post on the cost of communications that is a good, unesseray long introduction for advices 1. and 2. I also strongly advise watching this brilliant talk on communication-avoiding algorithms . All sample code and ncu reports can be found here along with compilation and execution instructions. The reports were ran on my Nvidia RTX 6000 Ada generation GPU. Lastly, here is a refresher on the separation of hardware and software concepts when programming Nvidia GPUs with CUDA: Figure 8: Sofware / harware memory hierarchy of an Nvidia GPU. source . The GPU hardawre is organized as follows: A GPU is made of an aggregation of Streaming Multiprocessors (SM) that contains the computational units. This is where the computations are done. Instructions are scheduled as so-called warps of size 32. The DRAM (slowest memory access, limited by the bandwith), accessible by all Streaming Multiprocessors (SM), the L2 cache, much smaller than the DRAM but faster, accessible visible by all SMs, One L1 cache by SM, much smaller than the L2 but faster, A register file per SM, much smaller than the L1 but the fastest. The software is organized as follows: Threads are uniquely defined sequences of operations defined by the CUDA kernels. They reside on the SM's. Blocks of threads. Threads of a block only reside in the same SM. Global memory. Visible by all threads, may reside in DRAM, L2 or L1 (potentially very slow !) Shared memory. Visible by all threads of a block, managed by the developer. Resides in L1. Local memory. Private to a thread, may reside in DRAM, L2 or L1 (potentially very slow !). Local memory includes both register spilling and stack usage. More on that in section 2. Registers. Private to a thread, resides in the SM's register file (the fastest !). Detect and avoid stack usage 1. Minimise redundant global memory accesses On an Nvidia GPU, any request to global memory may end up fetching data from: The DRAM, visible by all threads The L2 cache, visible by all threads The L1 cache, visible by all threads of a block As discussed in my post on the cost of communications , on recent GPUs (V/A/H/B100) it takes 50-100x more time to load a non cached FP64 double from DRAM up to registers than computing a FMA math operation on that number. I call this ratio the flop per load (FPL). The cache hierachy does mitigates that number. If the L1 cache hits, the memory request will be much faster, with a FPL of \"only\" 2-5x. For the L2 cache, the factor is about 5-10x. But it remains that every access to global memory is more expensive than a register manipulation by a factor of at least 2-5x. Thus you should avoid them at all cost . 1. Minimise redundant thread-level global memory accesses A first simple example, temporary register storages Let's considers at sample-1.cpp where we create two Device views: If you are not familiar with Kokkos views, in this context, they are just containers for vectors that resides on the GPU. const int size = 1<<27; Kokkos::View<float*> A(\"A\", size); Kokkos::View<float*> B(\"B\", size); and perform the following, rather silly, kernel, Kokkos::parallel_for(\"Kernel\", size, KOKKOS_LAMBDA(const int i) { for (int k=0; k<10; k++){ A(i) += B(i-1); A(i) += B(i); A(i) += B(i+1); } }); The issue is probably already striking to you: each instance of A(i) += is a global memory R/W. The solution is straightforward and found in sample-1-fixed.cpp : use a temporary storage for A , and R/W only once per thread: Kokkos::parallel_for(\"Kernel\", size, KOKKOS_LAMBDA(const int i) { float tmp=0; for (int k=0; k<10; k++){ tmp += B(i-1); tmp += B(i); tmp += B(i+1); } A(i) += tmp; }); Note : Since each thread uses the same values of B as it's neighbors, shared memory could be used to further improve performance. However, this kernel is simple enough so that caches probably already do a enough good job. With this simple change, we went from 60R, 30W per thread to 31R, 1W. You might think that it is such an obvious thing to avoid that it is not even worth talking about it. But I disagree ! Often, when first porting to kokkos, in a time limited environment, we simply replace the e.g. std::vector by Kokkos::View in the kernel body, check functionnality and move onto the next kernel, resulting in this issue hindering performance. Moreover, for more very long, intricate kernels with many Views, spotting and removing redundant memory accesses is quite tedious. Try for e.g. this one . Profiler diagnosis Let's look into the profiler report for the first sample code (Download it and load it in ncu ! Works even if you don't have a Nvidia GPU), sample-1.ncu-rep . First, let's look at the GPU SOL section: Figure 1: GPU SOL section for sample-1.cpp. We can see that the memory is heavily used. This begs the question, are we using it effectively ? Let's go to the memory workload analysis and dissect some elements Figure 2: Memory workload analysis for sample-1.cpp. Memory Throughput [Gbyte/s] 308.75 --> This is much lower than my GPU's bandwidth of 960.0, Communications between DRAM (Device Memory) and L2: 1.07 GB reads, which corresponds to \\(2^{27}(\\text{size}) \\times 2 (\\text{A and B}) \\times 4 (\\text{bytes per double})=1.07\\times 10^9\\) bytes. There is half as much writes, corresponding to A being modified. Both A and B are loaded once into the L2 cache, and A is written back only once into DRAM Good ! Communications between L2 and L1: About as much reads into L1, a little more probably due to cache misses. But, an astounding 9.14 GB of data written from L1 to L2, due to cache invalidations ! This is a great hint of redundant memory accesses; a big discrepancy between expected and observed volumes of exchanges. Essentially, this is the cache that is working hard to save you from you own mistakes, by not writing back all the way to DRAM at each A(i)+= . It really is saving you, as if we switch to throuput view, we see that these excessinve writes are done at an astounding 1.89 TB/s, twice as fast as my GPU's bandwith ! Lastly, let's look at one of my favorite sections, the warp state statistics session, espacially the warp states: Figure 3: Warp States for sample-1.cpp. If you are not familiar with warps states, really consider looking at the 1h30 lecture by Athena Elfarou . Essentially, a warp is a group of 32 threads (32 instances of the kernel with neigbouring index i ). It can be either: Stalled: waiting on a dependancy: for instance, to perform A(i)+=B(i) , you need to wait that both values were loaded from global memory into the registers. This takes a while. You can also be waiting on a math operation to be done, a barrier, or the execution queue to have a spot. Eligible: available to be scheduled, Selected: will issue an instruction on the next cycle. The warp states shows you the reasons why your warps have been stalled during the execution of your kernel, sorted by importance. This is precisely what you should worry about ! We see two reasons here with quite obscure names \"Stall long scoreboard and Stall LG throttle\". You can drag over your mouse onto the items to get an explanation, e.g. for the first one: Figure 4: Metric information for Stall long scoreboard. Stall long scoreboard means that warps are waiting on a memory dependancy from global memory, this not surprising and a very common one for memory bound kernels. Stall LG throttle means that the warps are waiting on the warp slot queue to have a spot to be scheduled. Indeed, each warp scheduler has a finite amount of spots for it's warps to be scheduled. If a kernel issues too many requests, warps are waiting, not on a dependancy, but simply on a spot in the queue. This is also a good symptom of redundant memory operations ! Let't now take a look at sample-1-fixed.ncu-rep . I recommend using the \"add baseline\" functionnality, so that we can track our progress ! First thing you can notice is that we get a huge performance gain: from 5.08ms to 1.81ms, a 64% speedup ! Then, going into the several sections: GPU Speed of light throuput: The compute pipeline is less busy, I'm honestly not sure why. The memory pipeline is more used (+6%) Figure 5: Memory workload analysis for sample-1-fixed.cpp. Memory workload Analysis: The memory throuput is much closer to the theoretical peak (865 GB/s, +180%) The previously healthy memory transfers are unchanged, but the L1 to L2 writes are reduced by 94%, as well as the caches hit rates. This shows that our cleaner implementation relies less on the caches, because it has much fewer redundant memory accesses. Static arrays as temporary storages Let's now consider a multi-dimensional case, with 2D Views and at least one run-time axis size, here, dim : const int size = 1<<27; int dim = 3; Kokkos::View<float**> A(\"A\", size, dim); Kokkos::View<float**> B(\"B\", size, dim); and the following kernel from sample-2.cpp : Kokkos::parallel_for(\"Kernel\", size, KOKKOS_LAMBDA(const int i) { for (int k = 0; k < 10; k++){ for (int dir = 0; dir < dim; dir++){ for (int dir2 = 0; dir2 < dim; dir2++){ A(i,dir) += B(i,dir2); } } } }); It is clear that there are redundant memory accesses, that we would like to store in a array residing in registers as follows: Kokkos::parallel_for(\"Kernel\", size, KOKKOS_LAMBDA(const int i) { float Atmp[dim], Btmp[dim]; for (int dir = 0; dir < dim; dir++){ Atmp[dir] = A(i, dir); Btmp[dir] = B(i, dir); } for (int k = 0; k < 10; k++){ for (int dir = 0; dir < dim; dir++){ for (int dir2 = 0; dir2 < dim; dir2++){ Atmp[dir] += Btmp[dir]; } } } for (int dir = 0; dir < dim; dir++){ A(i,dir) = Atmp[dir]; } }); However, dim is a run-time variable, while arrays must be declared with compile time-value. In this case, this generate a compile error. If you somehow manage to get a run-time sized vector in your kernel and avoid a compilation error, you will at best get a slow runtime, as it will reside in local memory, cf. section 2. The solution is to wrap the kernel in a dim -templated function, as done in sample-2-fixed.cpp : template<int dim> void apply_kernel(Kokkos::View<float**> A, Kokkos::View<float**> B, int size){ Kokkos::parallel_for(\"Kernel\", size, KOKKOS_LAMBDA(const int i) { // ... the previous kernel ... }); } which can be called as follows, where dim is picked at runtime: if (dim==1){ apply_kernel<1>(A, B, size); } else if (dim==2){ apply_kernel<2>(A, B, size); } else if (dim==3){ apply_kernel<3>(A, B, size); } else{ // Need more instantiations ! Fail or warning } I will not go through the ncu reports for this second example as the behavior is really similar to sample-1, but feel free to look a them yourself. The speedup obtained is 75% which is not surpising, since I pick the examples. Minimize block-level redundant memory accesses: shared memory If you spot that nighbouring threads (that likely reside on the same block) are using extensively the same elements from global memory, I strongly suggest you learn more about shared-memory , to reduce redundant block-level memory accesses. shared-memory is essentially a portion of the L1 cache managed by the user. L1 cache is the fastest cache, right before registers, that is shared by threads of a block. But beware, using it means that you think you can do a better job than the runtime ! :). To use shared memory in Kokkos, look at Hierarchical Parallelism . Minimise redundant kernel-level memory accesses: coalescing sectors and cache line When using Nvidia GPUs, threads are packed in so-called warps of 32 threads which execute instructions simultaneously (SIMD). In the same way, when a memory request if performed, (like load one FP64 number from global memory), it is not done for a single one, but in packs of so-called sectors of 32 bytes (i.e. 4 FP64 numbers). Another way to say this is that the memory access granularity of the GPU is of 32 bytes. As a result, the best case scenario for a warp load, is that it requires data that is coalescing: Figure 6: Coalesced memory accesses Source . In this ideal case, each thread is loading a FP64 number. There are 32 threads in a warps so this amount to 32 FP64 number, e.g. 256 bytes which is 8 sectors. This is very efficient because 256 bytes are loaded, and 256 bytes are used: the bandwith of the GPU is fully used. Let's now look at the worst case scenario: Figure 7: strided memory accesses. Adapted from source . In this case, each thread requires is also loading a FP64 numbers, but there is a sector-wide stride between threads. Since a FP64 number cannot be loaded \"on it's own\", a whole sector is loaded for each. As a result 32 sectors = 1024 bytes are loaded, for only 256 bytes used. This means that only a quarter of the bandwitdh is used, and the case gets worst if you are working with smaller datatypes. Profiler diagnosis The ratio of bytes loaded to bytes used per memory request is actually shown in the memoy workload analysis. For example, for sample-1-fixed.ncu-rep , we can see: Figure 5: DRAM Global Load Access Pattern warning for sample-1-fixed.cpp . where we see that for each 32 bytes sector transmitted, \"only\" 28.4 are used. This is pretty good of course as this code is very simple. But for more complicated operations, such as numerical simulation on unstructured meshes, this can be very bad. This section of ncu is helpful to detect that precise issue. Advices These observations should make you want to think about your data layout. In particular, you should organize your data so that neighbouring threads are working on neighbouring data . This means that there is not one good general answer on which storage is better for a dense matrix, row-major or column-major. If you are doing matrix-vector product, sure, row-major is better as it will be more cache friendly. If you do matrix-transpose-vector product, you want column-major. In the same way, there is no definitive better answer to the debate Array of Structure vs. Structure of Array. It all depends on what you will be doing with the you data, how you will iterate over it. In other words, match your data layout with your iteration layout . You will hear people say things like \"LayoutLeft (column-major) is best on GPU, LayoutRight (row-major) is best on CPU\" . This is not a general truth. Let's look at an example: Consider a matrix A of size, NxN and the following two kernels: compute col , a vector of N elements defined as the sum of each columns of A , by assigning each column to a thread, compute row , a vector of N elements defined as the sum of each rows of A , by assigning each row to a thread. Note: This is quite a bad way to parallelize this computation. For kernel 1, storing A as column-major is better. Each thread is accessing a column which is a coalesced memory segment. If one uses row-major storage, we can expect terrible performance. The opposite argument applies for kernel 2. It will be fast for LayoutRight storage, and slow for LayoutLeft. In sample-3.cpp , we perform kernels 1 and 2 on a LayoutLeft Kokkos View: int N = 10000; Kokkos::View<float**, Kokkos::LayoutLeft> A_LL(\"A_LL\", N, N); Kokkos::View<float**, Kokkos::LayoutRight> A_LR(\"A_LR\", N, N); Kokkos::View<float*> row_sum(\"row_sum\", N); // [...] Kokkos::parallel_for(\"RowSumLL\", N, KOKKOS_LAMBDA(int i) { float sum = 0.0; for (int j = 0; j < N; j++) { sum += A_LL(i,j); } row_sum(i) = sum; }); Kokkos::parallel_for(\"RowSumLR\", N, KOKKOS_LAMBDA(int i) { float sum = 0.0; for (int j = 0; j < N; j++) { sum += A_LR(i,j); } row_sum(i) = sum; }); The report can be found at sample-3.ncu-rep in which we can see that: ColSumLL runs in 0.52 ms, RowSumLL runs in 1.14ms (+120%), and the memory workload analysis section says: \"On average, only 4.0 of the 32 bytes transmitted per sector are utilized by each thread\" . Moreover, the L1 cache is used a lot, whereas ColSumLL does not use it at all. As you see, the best layout depends on what operation you plan on doing on your data, and how you plan on doing it. Another example is matrix multiplication where the best layout is cache/register-fitting blocked layout, because the best algorithm is cache/register-fitting blocked matrix multiplication. If you were to implement the naive version of matrix multiply C=AB where, for each element of C , you load a row of A and a column of B and perform the dot product, the best layout is storing A/B in row/column-major order respectively. For simulations on unstructured meshes, I recommend using Z-order curve re-ordering of the mesh-element. For our CFD code TRUST, this enabled an overall +20% speedup due to a better coalescing. Note: Non-coalesced write are worst in performance that non-coalesced read, as a write needs to invalidate caches, we say that writes are \"cache-through\". A single non-coalesced write can invalidate a sector in L1, and L2, requireing the data to be fetched potentially all the way from DRAM for the next load. Note: The default layout for multidimennal views in Kokkos is LayoutLeft on device, and LayoutRight on host. I believe this is due to historical reasons; Algorithms from the Trilinos library that is built upon Kokkos runs more efficiently this way. But again, this is application-specific. 2. Avoid the use of local memory What is local memory. As we saw in the introduction, local memory is private to a thread and may reside in DRAM, L2 or L1, which is potentially very slow, and in any case much slower than registers. Local memory can be used in two cases: register spilling stack usage. Which correspond Detect and avoid stack usage ref a la precedente section How to avoid stack usage attention aux tableaux statiques Detect and avoid register spilling Hide latency The occupancy trap, ILP, hide latency reduce Register usage Reduce usage, launch bound, no MDRANGE, we dont talk about block and share memory limits, template away expensive branches Why does it spills 3. Minimize redundant math operation, use cheap arithmetics Background Profiler diagnosis Advices FMA, / vs *, unroll loop for int computation might need to template Math can be a bottleneck Do smarter math 4. Understanding occupancy Background Profiler diagnosis Advices 6. Avoid thread divergence Background Profiler diagnosis Advices The SIMD pattern, masking templating Final advices Participate to hackathons !","title":"(WIP) Six basic performance advices for porting kernels to the GPU"},{"location":"posts/post1/#wip-six-basic-performance-advices-for-porting-kernels-to-the-gpu","text":"","title":"(WIP) Six basic performance advices for porting kernels to the GPU."},{"location":"posts/post1/#0-introduction","text":"","title":"0. Introduction"},{"location":"posts/post1/#some-context-and-motivations","text":"I was hired by CEA to join the porting effort of this legacy code to the GPU using Kokkos . This is quite a challenging task as the code is 20 years old, and more than 1400 kernels were identified to be ported to the GPU ! As I went and optimized some kernels, something struck me: The nature of the task of porting code to the GPU, especially when time is limited, often lead to small mistakes that can undermine performance. The goal of this blogpost is to give you basic , easy tips to keep in mind when writing / porting / first optimizing your kernels, so that you get a reasonable performance. By applying them, I was able to get the following speedups that are measured relative to an already GPU-enabled baseline: A 40-50% speedup on a CFD convection kernel from TRUST (obtained on RTX A5000, RTX A6000 Ada and H100 GPUs). Brace yourself : this is a monstruous kernel. A 20-50% speedup on a CFD diffusion kernel from TRUST (obtained on RTX A6000 Ada and H100 GPUs). A 20% speedup on a MUSCL reconstruction kernel from the radiative hydrodynamics code heraclescpp (obtained on a A100 GPU) By reasonable I do not mean that you will get optimal performance. In fact, I will not go over what I consider to be advanced optimization advices such as the use of shared memory , vectorized operations , tensor cores operations , hardware-specific optimizations . If getting optimal performance is crucial to your application, consider learning more and apply these, but keep in mind that performance often comes at the cost of portability . The advices are general enough so that they should allow speedups on all cards from all vendors. By advanced , I do not mean that these topics are especially difficult or out of reach, but only that they require a significant design effort to be used effectively in a production context such as a wide CFD code like TRUST. In contrast, I believe that the advices I will give to you in this blogpost are easy enough so that you can, and should apply them straightforwardly while porting your code to the GPU in a time limited environement. Note 1: The target audience is engineer / researcher that want to get started with GPU porting in a code that relies on custom, domain specific low-level kernel. But do not reinvent the wheel ! i.e. do not rewrite kernels that have been implemented, hihgly optimized and distributed in libraries. Consider looking into (non exhaustive list !): CUDA Libraries . kokkos kernels for portable BLAS, sparse BLAS and fraph kernels. Trilinos for high level, portable solutions for the solution of large-scale, complex multi-physics engineering and scientific problems. PETSc for the scalable solution of scientific applications modeled by partial differential equations (PDEs)","title":"Some context and motivations"},{"location":"posts/post1/#disclaimers","text":"If you think I wrote something that is wrong, or misleading please let me know ! I am running my performance tests on Nvidia GPUs, just because they are more easily available to me, and that I am more familiar with the performance tools such as nsight systems (nsys) and nsight compute (ncu). However, note that AMD provides similar profilers (altough, at the time I am writing this, rocm's kernel profilers seem a lot less user friendly), and that the advices that I give here are general enought so that they apply for GPUs from both vendors. Moreover, I will use Kokkos as the programming model for the code sample, just because I work with it, and that performance portability is cool . Again, the concepts are simple enought so that you can translate them to your favorite programming model, OpenMP, SYCL, Cuda, Hip.","title":"Disclaimers"},{"location":"posts/post1/#pre-requisits","text":"In this small tutorial, I will assume that you are already familiar with: Basic C++. The reason why you might want to use the GPU, and that you need a big enough problem to make full use of it. Some understanding of GPU performance: The roofline performance model. What does compute bound / memory bound mean. Basic GPU architecture, in particular: Some knowledge about the memory hierarchy (registers, L1/L2 caches, DRAM) and the increasing cost of memory accesses. What are CUDA threads / blocks, global memory. You can be confused about what is local memory . Some ressources on GPU architecture / CUDA programming: 1h30 lecture by Athena Elfarou 13 lectures by Bob Crovella Altough not necessary for getting through this post, I recommend you learn about: How to compile a GPU code, generate a report with Nvidia nsight compute and loading in with the ui. Application-level optimization: How to build a sensible optimization roadmap with e.g. Nvidia Nsight System That you should avoid host to device memory transfers; This tutorial is centered on kernel-level optimisation. We assume memory is already available on the GPU How to ensure that it is worth it to optimize the kernel you are looking (Don't assume bottleneck, profile, assess, optimize). Some ressouces: 8th lecture from the Bob Crovella lecture series which focuses on that topic. What is Kokkos, why you might want to use it and how to get started with it. Some ressources: Talk by Christian Trott, Co-leader of the Kokkos core team . Kokkos lecture series (kind of outdated, but you can find a lot of ressources online, alos, join the slack !). Note: you really should consider using Kokkos, or any other portable programming model. It's good enough so that CEA adopted it for it's legacy codes ! (see the CExA project ).","title":"Pre-requisits"},{"location":"posts/post1/#outline","text":"The outline for this post is the following six rules of thumbs,or advices, largely inspired by the Nvidia Ampere tuning guide : Minimize redundant global memory accesses. Minimize redundant math operation, use cheap arithmetics. Understanding occupancy. Avoid the use of Local memory . Avoid thread divergence. Feel free to jump straight into your sections of interest. One of the main interest of this tutorial is to teach you where to look for information in ncu. Look for \"Profiler diagnosis\" sections.","title":"Outline"},{"location":"posts/post1/#before-we-start","text":"Before going into the 6 advices, I invite you to read my post on the cost of communications that is a good, unesseray long introduction for advices 1. and 2. I also strongly advise watching this brilliant talk on communication-avoiding algorithms . All sample code and ncu reports can be found here along with compilation and execution instructions. The reports were ran on my Nvidia RTX 6000 Ada generation GPU. Lastly, here is a refresher on the separation of hardware and software concepts when programming Nvidia GPUs with CUDA: Figure 8: Sofware / harware memory hierarchy of an Nvidia GPU. source . The GPU hardawre is organized as follows: A GPU is made of an aggregation of Streaming Multiprocessors (SM) that contains the computational units. This is where the computations are done. Instructions are scheduled as so-called warps of size 32. The DRAM (slowest memory access, limited by the bandwith), accessible by all Streaming Multiprocessors (SM), the L2 cache, much smaller than the DRAM but faster, accessible visible by all SMs, One L1 cache by SM, much smaller than the L2 but faster, A register file per SM, much smaller than the L1 but the fastest. The software is organized as follows: Threads are uniquely defined sequences of operations defined by the CUDA kernels. They reside on the SM's. Blocks of threads. Threads of a block only reside in the same SM. Global memory. Visible by all threads, may reside in DRAM, L2 or L1 (potentially very slow !) Shared memory. Visible by all threads of a block, managed by the developer. Resides in L1. Local memory. Private to a thread, may reside in DRAM, L2 or L1 (potentially very slow !). Local memory includes both register spilling and stack usage. More on that in section 2. Registers. Private to a thread, resides in the SM's register file (the fastest !).","title":"Before we start"},{"location":"posts/post1/#detect-and-avoid-stack-usage","text":"","title":"Detect and avoid stack usage"},{"location":"posts/post1/#1-minimise-redundant-global-memory-accesses","text":"On an Nvidia GPU, any request to global memory may end up fetching data from: The DRAM, visible by all threads The L2 cache, visible by all threads The L1 cache, visible by all threads of a block As discussed in my post on the cost of communications , on recent GPUs (V/A/H/B100) it takes 50-100x more time to load a non cached FP64 double from DRAM up to registers than computing a FMA math operation on that number. I call this ratio the flop per load (FPL). The cache hierachy does mitigates that number. If the L1 cache hits, the memory request will be much faster, with a FPL of \"only\" 2-5x. For the L2 cache, the factor is about 5-10x. But it remains that every access to global memory is more expensive than a register manipulation by a factor of at least 2-5x. Thus you should avoid them at all cost .","title":"1. Minimise redundant global memory accesses"},{"location":"posts/post1/#1-minimise-redundant-thread-level-global-memory-accesses","text":"","title":"1. Minimise redundant thread-level global memory accesses"},{"location":"posts/post1/#a-first-simple-example-temporary-register-storages","text":"Let's considers at sample-1.cpp where we create two Device views: If you are not familiar with Kokkos views, in this context, they are just containers for vectors that resides on the GPU. const int size = 1<<27; Kokkos::View<float*> A(\"A\", size); Kokkos::View<float*> B(\"B\", size); and perform the following, rather silly, kernel, Kokkos::parallel_for(\"Kernel\", size, KOKKOS_LAMBDA(const int i) { for (int k=0; k<10; k++){ A(i) += B(i-1); A(i) += B(i); A(i) += B(i+1); } }); The issue is probably already striking to you: each instance of A(i) += is a global memory R/W. The solution is straightforward and found in sample-1-fixed.cpp : use a temporary storage for A , and R/W only once per thread: Kokkos::parallel_for(\"Kernel\", size, KOKKOS_LAMBDA(const int i) { float tmp=0; for (int k=0; k<10; k++){ tmp += B(i-1); tmp += B(i); tmp += B(i+1); } A(i) += tmp; }); Note : Since each thread uses the same values of B as it's neighbors, shared memory could be used to further improve performance. However, this kernel is simple enough so that caches probably already do a enough good job. With this simple change, we went from 60R, 30W per thread to 31R, 1W. You might think that it is such an obvious thing to avoid that it is not even worth talking about it. But I disagree ! Often, when first porting to kokkos, in a time limited environment, we simply replace the e.g. std::vector by Kokkos::View in the kernel body, check functionnality and move onto the next kernel, resulting in this issue hindering performance. Moreover, for more very long, intricate kernels with many Views, spotting and removing redundant memory accesses is quite tedious. Try for e.g. this one .","title":"A first simple example, temporary register storages"},{"location":"posts/post1/#profiler-diagnosis","text":"Let's look into the profiler report for the first sample code (Download it and load it in ncu ! Works even if you don't have a Nvidia GPU), sample-1.ncu-rep . First, let's look at the GPU SOL section: Figure 1: GPU SOL section for sample-1.cpp. We can see that the memory is heavily used. This begs the question, are we using it effectively ? Let's go to the memory workload analysis and dissect some elements Figure 2: Memory workload analysis for sample-1.cpp. Memory Throughput [Gbyte/s] 308.75 --> This is much lower than my GPU's bandwidth of 960.0, Communications between DRAM (Device Memory) and L2: 1.07 GB reads, which corresponds to \\(2^{27}(\\text{size}) \\times 2 (\\text{A and B}) \\times 4 (\\text{bytes per double})=1.07\\times 10^9\\) bytes. There is half as much writes, corresponding to A being modified. Both A and B are loaded once into the L2 cache, and A is written back only once into DRAM Good ! Communications between L2 and L1: About as much reads into L1, a little more probably due to cache misses. But, an astounding 9.14 GB of data written from L1 to L2, due to cache invalidations ! This is a great hint of redundant memory accesses; a big discrepancy between expected and observed volumes of exchanges. Essentially, this is the cache that is working hard to save you from you own mistakes, by not writing back all the way to DRAM at each A(i)+= . It really is saving you, as if we switch to throuput view, we see that these excessinve writes are done at an astounding 1.89 TB/s, twice as fast as my GPU's bandwith ! Lastly, let's look at one of my favorite sections, the warp state statistics session, espacially the warp states: Figure 3: Warp States for sample-1.cpp. If you are not familiar with warps states, really consider looking at the 1h30 lecture by Athena Elfarou . Essentially, a warp is a group of 32 threads (32 instances of the kernel with neigbouring index i ). It can be either: Stalled: waiting on a dependancy: for instance, to perform A(i)+=B(i) , you need to wait that both values were loaded from global memory into the registers. This takes a while. You can also be waiting on a math operation to be done, a barrier, or the execution queue to have a spot. Eligible: available to be scheduled, Selected: will issue an instruction on the next cycle. The warp states shows you the reasons why your warps have been stalled during the execution of your kernel, sorted by importance. This is precisely what you should worry about ! We see two reasons here with quite obscure names \"Stall long scoreboard and Stall LG throttle\". You can drag over your mouse onto the items to get an explanation, e.g. for the first one: Figure 4: Metric information for Stall long scoreboard. Stall long scoreboard means that warps are waiting on a memory dependancy from global memory, this not surprising and a very common one for memory bound kernels. Stall LG throttle means that the warps are waiting on the warp slot queue to have a spot to be scheduled. Indeed, each warp scheduler has a finite amount of spots for it's warps to be scheduled. If a kernel issues too many requests, warps are waiting, not on a dependancy, but simply on a spot in the queue. This is also a good symptom of redundant memory operations ! Let't now take a look at sample-1-fixed.ncu-rep . I recommend using the \"add baseline\" functionnality, so that we can track our progress ! First thing you can notice is that we get a huge performance gain: from 5.08ms to 1.81ms, a 64% speedup ! Then, going into the several sections: GPU Speed of light throuput: The compute pipeline is less busy, I'm honestly not sure why. The memory pipeline is more used (+6%) Figure 5: Memory workload analysis for sample-1-fixed.cpp. Memory workload Analysis: The memory throuput is much closer to the theoretical peak (865 GB/s, +180%) The previously healthy memory transfers are unchanged, but the L1 to L2 writes are reduced by 94%, as well as the caches hit rates. This shows that our cleaner implementation relies less on the caches, because it has much fewer redundant memory accesses.","title":"Profiler diagnosis"},{"location":"posts/post1/#static-arrays-as-temporary-storages","text":"Let's now consider a multi-dimensional case, with 2D Views and at least one run-time axis size, here, dim : const int size = 1<<27; int dim = 3; Kokkos::View<float**> A(\"A\", size, dim); Kokkos::View<float**> B(\"B\", size, dim); and the following kernel from sample-2.cpp : Kokkos::parallel_for(\"Kernel\", size, KOKKOS_LAMBDA(const int i) { for (int k = 0; k < 10; k++){ for (int dir = 0; dir < dim; dir++){ for (int dir2 = 0; dir2 < dim; dir2++){ A(i,dir) += B(i,dir2); } } } }); It is clear that there are redundant memory accesses, that we would like to store in a array residing in registers as follows: Kokkos::parallel_for(\"Kernel\", size, KOKKOS_LAMBDA(const int i) { float Atmp[dim], Btmp[dim]; for (int dir = 0; dir < dim; dir++){ Atmp[dir] = A(i, dir); Btmp[dir] = B(i, dir); } for (int k = 0; k < 10; k++){ for (int dir = 0; dir < dim; dir++){ for (int dir2 = 0; dir2 < dim; dir2++){ Atmp[dir] += Btmp[dir]; } } } for (int dir = 0; dir < dim; dir++){ A(i,dir) = Atmp[dir]; } }); However, dim is a run-time variable, while arrays must be declared with compile time-value. In this case, this generate a compile error. If you somehow manage to get a run-time sized vector in your kernel and avoid a compilation error, you will at best get a slow runtime, as it will reside in local memory, cf. section 2. The solution is to wrap the kernel in a dim -templated function, as done in sample-2-fixed.cpp : template<int dim> void apply_kernel(Kokkos::View<float**> A, Kokkos::View<float**> B, int size){ Kokkos::parallel_for(\"Kernel\", size, KOKKOS_LAMBDA(const int i) { // ... the previous kernel ... }); } which can be called as follows, where dim is picked at runtime: if (dim==1){ apply_kernel<1>(A, B, size); } else if (dim==2){ apply_kernel<2>(A, B, size); } else if (dim==3){ apply_kernel<3>(A, B, size); } else{ // Need more instantiations ! Fail or warning } I will not go through the ncu reports for this second example as the behavior is really similar to sample-1, but feel free to look a them yourself. The speedup obtained is 75% which is not surpising, since I pick the examples.","title":"Static arrays as temporary storages"},{"location":"posts/post1/#minimize-block-level-redundant-memory-accesses-shared-memory","text":"If you spot that nighbouring threads (that likely reside on the same block) are using extensively the same elements from global memory, I strongly suggest you learn more about shared-memory , to reduce redundant block-level memory accesses. shared-memory is essentially a portion of the L1 cache managed by the user. L1 cache is the fastest cache, right before registers, that is shared by threads of a block. But beware, using it means that you think you can do a better job than the runtime ! :). To use shared memory in Kokkos, look at Hierarchical Parallelism .","title":"Minimize block-level redundant memory accesses: shared memory"},{"location":"posts/post1/#minimise-redundant-kernel-level-memory-accesses-coalescing","text":"","title":"Minimise redundant kernel-level memory accesses: coalescing"},{"location":"posts/post1/#sectors-and-cache-line","text":"When using Nvidia GPUs, threads are packed in so-called warps of 32 threads which execute instructions simultaneously (SIMD). In the same way, when a memory request if performed, (like load one FP64 number from global memory), it is not done for a single one, but in packs of so-called sectors of 32 bytes (i.e. 4 FP64 numbers). Another way to say this is that the memory access granularity of the GPU is of 32 bytes. As a result, the best case scenario for a warp load, is that it requires data that is coalescing: Figure 6: Coalesced memory accesses Source . In this ideal case, each thread is loading a FP64 number. There are 32 threads in a warps so this amount to 32 FP64 number, e.g. 256 bytes which is 8 sectors. This is very efficient because 256 bytes are loaded, and 256 bytes are used: the bandwith of the GPU is fully used. Let's now look at the worst case scenario: Figure 7: strided memory accesses. Adapted from source . In this case, each thread requires is also loading a FP64 numbers, but there is a sector-wide stride between threads. Since a FP64 number cannot be loaded \"on it's own\", a whole sector is loaded for each. As a result 32 sectors = 1024 bytes are loaded, for only 256 bytes used. This means that only a quarter of the bandwitdh is used, and the case gets worst if you are working with smaller datatypes.","title":"sectors and cache line"},{"location":"posts/post1/#profiler-diagnosis_1","text":"The ratio of bytes loaded to bytes used per memory request is actually shown in the memoy workload analysis. For example, for sample-1-fixed.ncu-rep , we can see: Figure 5: DRAM Global Load Access Pattern warning for sample-1-fixed.cpp . where we see that for each 32 bytes sector transmitted, \"only\" 28.4 are used. This is pretty good of course as this code is very simple. But for more complicated operations, such as numerical simulation on unstructured meshes, this can be very bad. This section of ncu is helpful to detect that precise issue.","title":"Profiler diagnosis"},{"location":"posts/post1/#advices","text":"These observations should make you want to think about your data layout. In particular, you should organize your data so that neighbouring threads are working on neighbouring data . This means that there is not one good general answer on which storage is better for a dense matrix, row-major or column-major. If you are doing matrix-vector product, sure, row-major is better as it will be more cache friendly. If you do matrix-transpose-vector product, you want column-major. In the same way, there is no definitive better answer to the debate Array of Structure vs. Structure of Array. It all depends on what you will be doing with the you data, how you will iterate over it. In other words, match your data layout with your iteration layout . You will hear people say things like \"LayoutLeft (column-major) is best on GPU, LayoutRight (row-major) is best on CPU\" . This is not a general truth. Let's look at an example: Consider a matrix A of size, NxN and the following two kernels: compute col , a vector of N elements defined as the sum of each columns of A , by assigning each column to a thread, compute row , a vector of N elements defined as the sum of each rows of A , by assigning each row to a thread. Note: This is quite a bad way to parallelize this computation. For kernel 1, storing A as column-major is better. Each thread is accessing a column which is a coalesced memory segment. If one uses row-major storage, we can expect terrible performance. The opposite argument applies for kernel 2. It will be fast for LayoutRight storage, and slow for LayoutLeft. In sample-3.cpp , we perform kernels 1 and 2 on a LayoutLeft Kokkos View: int N = 10000; Kokkos::View<float**, Kokkos::LayoutLeft> A_LL(\"A_LL\", N, N); Kokkos::View<float**, Kokkos::LayoutRight> A_LR(\"A_LR\", N, N); Kokkos::View<float*> row_sum(\"row_sum\", N); // [...] Kokkos::parallel_for(\"RowSumLL\", N, KOKKOS_LAMBDA(int i) { float sum = 0.0; for (int j = 0; j < N; j++) { sum += A_LL(i,j); } row_sum(i) = sum; }); Kokkos::parallel_for(\"RowSumLR\", N, KOKKOS_LAMBDA(int i) { float sum = 0.0; for (int j = 0; j < N; j++) { sum += A_LR(i,j); } row_sum(i) = sum; }); The report can be found at sample-3.ncu-rep in which we can see that: ColSumLL runs in 0.52 ms, RowSumLL runs in 1.14ms (+120%), and the memory workload analysis section says: \"On average, only 4.0 of the 32 bytes transmitted per sector are utilized by each thread\" . Moreover, the L1 cache is used a lot, whereas ColSumLL does not use it at all. As you see, the best layout depends on what operation you plan on doing on your data, and how you plan on doing it. Another example is matrix multiplication where the best layout is cache/register-fitting blocked layout, because the best algorithm is cache/register-fitting blocked matrix multiplication. If you were to implement the naive version of matrix multiply C=AB where, for each element of C , you load a row of A and a column of B and perform the dot product, the best layout is storing A/B in row/column-major order respectively. For simulations on unstructured meshes, I recommend using Z-order curve re-ordering of the mesh-element. For our CFD code TRUST, this enabled an overall +20% speedup due to a better coalescing. Note: Non-coalesced write are worst in performance that non-coalesced read, as a write needs to invalidate caches, we say that writes are \"cache-through\". A single non-coalesced write can invalidate a sector in L1, and L2, requireing the data to be fetched potentially all the way from DRAM for the next load. Note: The default layout for multidimennal views in Kokkos is LayoutLeft on device, and LayoutRight on host. I believe this is due to historical reasons; Algorithms from the Trilinos library that is built upon Kokkos runs more efficiently this way. But again, this is application-specific.","title":"Advices"},{"location":"posts/post1/#2-avoid-the-use-of-local-memory","text":"","title":"2. Avoid the use of local memory"},{"location":"posts/post1/#what-is-local-memory","text":"As we saw in the introduction, local memory is private to a thread and may reside in DRAM, L2 or L1, which is potentially very slow, and in any case much slower than registers. Local memory can be used in two cases: register spilling stack usage. Which correspond","title":"What is local memory."},{"location":"posts/post1/#detect-and-avoid-stack-usage_1","text":"ref a la precedente section How to avoid stack usage attention aux tableaux statiques","title":"Detect and avoid stack usage"},{"location":"posts/post1/#detect-and-avoid-register-spilling","text":"Hide latency The occupancy trap, ILP, hide latency reduce Register usage Reduce usage, launch bound, no MDRANGE, we dont talk about block and share memory limits, template away expensive branches Why does it spills","title":"Detect and avoid register spilling"},{"location":"posts/post1/#3-minimize-redundant-math-operation-use-cheap-arithmetics","text":"","title":"3. Minimize redundant math operation, use cheap arithmetics"},{"location":"posts/post1/#background","text":"","title":"Background"},{"location":"posts/post1/#profiler-diagnosis_2","text":"","title":"Profiler diagnosis"},{"location":"posts/post1/#advices_1","text":"FMA, / vs *, unroll loop for int computation might need to template Math can be a bottleneck Do smarter math","title":"Advices"},{"location":"posts/post1/#4-understanding-occupancy","text":"","title":"4. Understanding occupancy"},{"location":"posts/post1/#background_1","text":"","title":"Background"},{"location":"posts/post1/#profiler-diagnosis_3","text":"","title":"Profiler diagnosis"},{"location":"posts/post1/#advices_2","text":"","title":"Advices"},{"location":"posts/post1/#6-avoid-thread-divergence","text":"","title":"6. Avoid thread divergence"},{"location":"posts/post1/#background_2","text":"","title":"Background"},{"location":"posts/post1/#profiler-diagnosis_4","text":"","title":"Profiler diagnosis"},{"location":"posts/post1/#advices_3","text":"The SIMD pattern, masking templating","title":"Advices"},{"location":"posts/post1/#final-advices","text":"Participate to hackathons !","title":"Final advices"},{"location":"posts/post2/","text":"The cost of communications Disclaimer This post was originally supposed to be a short introduction within my first post on GPU kernel optimization , but then I realized that I liked to talk too much about it and went out of scope. This is largely inspired by this brilliant talk by Prof. James Demmel (Berkley), as well as a the his CS267 class with free lectures on YouTube, where you can find everything that I explain here. Note The terms communications and memory transfers will be used interchangeably. Also in the present context, a word refers to a single FP64 number. Hardware trends Looking at Figure 1, memory transfers, within DRAM, or over the network, have been more expensive than (FP64) math operation since ~1992: Figure 1: Evolution of the time per flop (gamma), inverse bandwidth (beta) and latency (alpha) between ~1980 to ~2015. Source . The graph stops around 2015, where the ratio of gamma to beta (DRAM) was around 10. Let's look at the current FP64 Flop Per Load (FPL) factor for more recent hardware: GPU Release Year FP64 FLOPS (TFLOPS) BW (TB/s) FPL V100 2017 7.066 0.897 ~65.19 A100 2020 9.746 1.56 ~49.9 H100 2022 25.61 2.04 ~ 100 B200 2024 62 8.20 ~60 Table 1: Evolution of the BW, FP64 flops and FPL for recent Nvidia GPU models. FPL is computed as \\(\\frac{FP64 \\ Flops}{BW}*8\\) since a FP64 number is made of 8 bytes. Note It is illuminating to think about FPL as the average amount of FP64 operation you can do in the time that it takes to load one (non-cached) word from DRAM. As we can see, the FPL has been varying between 50 and 100. This is really large, and should motivate you to think really hard about how you access memory. In fact, communication avoiding algorithms have been a very active and fruitful research area which lead to the development of the BLAS standard, the Lapack library, and much more. Some naming conventions I want to clarify some conventions, and push for the use of clear terminologies when talking about numerical algorithms. Let's take some time to realize that the following terms refer to very different things (inspired by Mark Hoemmen's PhD introduction ): An operation is a theoretical construction for data transformation e.g. matrix multiplication, finite-element projection, a time-step of a finite volume method, a linear system resolution. It exists in the realm of ideas, may have inserting properties but does not refer to specific way to achieve to the result. An algorithm: (for an operation) is a sequence of instructions, that may be written in pseudo-code, that describes a way how to get the result of an operation. Different algorithms may give different results (in exact arithmetic) for a given operation because of e.g. different convergence rate / error threshold. An implementation: (of an algorithm) is a concrete, existing piece of code that, well, implements an algorithm. It may be in done in any language. Different implementations should give the same results in exact arithmetic for a given algorithm, but can give different results in finite arithmetic because of e.g. floating point precision and hardware/runtime details. A simple memory model One result that I like a lot is the one presented in the second CS267 lecture where the following simple memory model is proposed: Assume a simple machine with just 2 levels of memory, fast and slow (think of e.g. DRAM / registers) and the following properties and notations: \\(M=\\) number of words that fits into fast memory, no latency (simplifying assumption), \\(t_m=\\) time per slow memory operation e.g. to move a word from fast to slow memory (inverse BW from Table 1 multiplied by 8 in our case since we are doing FP64 and ignoring latency), \\(t_f=\\) time per arithmetic operation i.e. the inverse of the FLOPS in Table 1. Assume an implementation of an algorithm, for which the runtime on the machine is characterized by: \\(m=\\) number of words moved between fast and slow memory to complete the algorithm, \\(f=\\) number of arithmetic operations to complete the algorithm, We can then define \\(CI_{\\text{runtime}}=\\frac{f}{m}\\) , a property of the runtime of an implementation of the algorithm that is called the computational intensity . It is the average number of flops per slow memory access. While the previously defined FPL factor, a property of the machine , is just given as \\(FPL_{\\text{hardware}}=\\frac{t_m}{t_f}\\) . Note: Nvidia GPUs have 4 levels of memory: DRAM, L2 and L1 caches, and registers. Each level has ~an order of magnitude difference in bandwidth. Some CPUs have 5 levels with an additional L3 cache. Real memory models are super complicated ! However, it is clear that memory hierarchies are omnipresent and that thinking hard about them helps both CPU and GPU performances. Getting good performance The minimum possible time for our algorithm is \\(t_{\\text{ideal}}=ft_f\\) , which is attained when the problem fits in fast memory ( \\(m<M\\) ) and no slow memory transaction are required. This implies that we don't read any initial data from slow memory nor store in it, this is never the case in practice. Let's compare this to the real time for a big enough problem \\(t_{\\text{real}}=ft_f+mt_m\\) which rewrites: \\(t_{\\text{real}}= t_{\\text{ideal}}(1+\\frac{FPL_{\\text{hardware}}}{CI_{\\text{runtime}}})\\) It is now clear that to get near optimal performance, we want to reduce the ratio \\(\\frac{FPL_{\\text{hardware}}}{CI_{\\text{runtime}}}\\) as much as possible. Since \\({FPL_{\\text{hardware}}}\\) is a property of the hardware, with a value ranging between 50 and 100 depending on the GPU considered, all we can do is try to reduce \\(\\frac{1}{CI_{\\text{runtime}}}=\\frac{m}{f}\\) , by trying to reuse the data we load as much as possible. Properties of the runtime vs. properties of an implementation I insist on using the terminology \"properties of the runtime (of an implementation (of an algorithm)) \". Indeed, in practice, the numbers \\(f\\) , \\(m\\) and \\(CI_{\\text{runtime}}=\\frac{f}{m}\\) should not be obtained by simply computing the ratio of how much memory should be touched, and how many operation should be done ideally, optimally for a given operation. Because most real problems do not fit in cache. Instead, these numbers are a property of how the algorithm is implemented, compiled and ran. Performance for an operation can vary dramatically between a naive and a smart algorithm. Performance of an algorithm can vary dramatically between a naive and a smart implementation. Performance of an implementation can vary dramatically between a \"release\" build and a \"debug\" build, and between an \"old, crippled, small-cached\" machine and a \"brand new shiny and expensive\" machine. A big chunk of the implementation work is to force the compile-runtime pipeline to deliver the values of \\(f\\) and \\(m\\) that you desire. In this sense, I find CPU optimization is harder than GPU optimization because the gap between the implementation and the runtime is wider. In GPU programming, you are writing native SIMD code, and you can event control the L1 cache via shared memory. For CPU programming, you cannot control the cache, and using SIMD instructions is a pain. You have to write code and hope that the compiler/runtime does a good job of doing what you want it to do. Also, the Nvidia profilers are just fantastic. But this could be (and probably is) an exposition bias from me. Let's consider the example of a very generic operation: dense matrix multiplication , \\(C=A.B\\) (the 1st homework of CS267 and topic of the 2nd and 3rd lectures). Note matrix-multiplication is such a constrained operation that it is difficult to differentiate the operation from the algorithm. In fact, the difference between algorithms is the way they handle cache. If three \\(n\\times n\\) matrices fits in fast memory, we know that that we need to load/store only \\(3n^2\\) words (2 matrix read, 1 matrix write) from slow memory, and perform \\(2n^3\\) operations (one dot product per element of C, each dot product being \\(n\\) multiply and \\(n\\) add) with a resulting \\(CI_{\\text{ideal}}^{\\text{matmul}}=\\frac{3n}{2}\\) . The bigger \\(n\\) is, the closer we get from ideal performance. However, as \\(n\\) grows, it is clear that the problem does not fit in fast memory ( \\(3n^2>M\\) eventually). A naive implementation of matrix multiply is: for i in range(n): #load A[i,:] from fast memory (n words) for j in range (n): #load B[:,j] from fast memory (n words) #read C[i,j] from fast memory (1 word) C[i,j] = dot(A[i,:], B[:,j]) #(2n operations) #store C[i,j] in slow memory (1 word) #Total: # m = n^2 + n^3 + 2n^2 --> n^3, # f = 2n^3. It has a computational intensity of \\(CI_{\\text{naive}}^{\\text{matmul}}=\\mathcal{O}(2)\\) , which is terrible (compared to the ideal value) ! On the other hand, the well-known blocked implementation that splits and iterates over \\(b\\times b\\) sub-blocks of the matrices has a computational intensity of \\(CI_{\\text{blocked}}^{\\text{matmul}}=\\mathcal{O}(b)\\) , assuming that the blocks fit in fast memory: #pick a block size b that fits in cache (3b^2<M) N=n/b #compute block size for i in range(N): for j in range (N): #load the ijth block from C into fast memory (bxb words) Cblock=... for k in range (N) #load the ijth block from A into fast memory (bxb words) Ablock=... #load the ijth block from B into fast memory (bxb words) Bblock=... Cblock += matmul(Ablock, Bblock) #naive matmul, or micro kernel that fits in cache / registers (2b^3 operations) #store the ijth block from C into slow memory (bxb words) #Total: # m = N^2( 2b^2 + N(2b^2) ) --> 2N^3b^2= 2n^3/b # f = N^3(2b^3) =2n^3 So, you might wonder, what should I do ? How do I know if there is a better algorithm ? Well, a theoretical upper bound on the computational intensity has been found and is given by \\(CI_{\\text{blocked}}^{\\text{matmul}}=\\mathcal{O}(\\sqrt{M})\\) , and if you ever write a new dense matmul implementation, you should strive to reach it. And notice ! the blocked algorithm reaches that bound. Indeed, since the blocks fit in fast memory, \\(3b^2 <M\\) \\(\\implies\\) \\(b=\\mathcal{O}(\\sqrt{M})\\) . This is the whole point of communication avoiding algorithms research: computing lower bounds and finding algorithm that reaches them. Again, if you find this interesting, consider looking at this brilliant introduction . Conclusion Well, all that is quite fascinating, but also overwhelming don't you think ? Well, you might not have to think about all this lower bound theory to get good speedups. In my first post on GPU kernel optimization I go over frequent coding mistakes that leads to extra useless communications. In that post, I will not give guidelines to reach theoretical lower bounds for your case. This is just too general to be discussed in a blog-post. As we saw, it constitutes a research topic on it's own and implies a deep re-thinking of the algorithms and data structures. No, here we will stay simple and focus on the following: given a GPU kernel, what frequent coding mistakes should we avoid to limit the amount of data we load/store from slow memory.","title":"The cost of communications"},{"location":"posts/post2/#the-cost-of-communications","text":"","title":"The cost of communications"},{"location":"posts/post2/#disclaimer","text":"This post was originally supposed to be a short introduction within my first post on GPU kernel optimization , but then I realized that I liked to talk too much about it and went out of scope. This is largely inspired by this brilliant talk by Prof. James Demmel (Berkley), as well as a the his CS267 class with free lectures on YouTube, where you can find everything that I explain here. Note The terms communications and memory transfers will be used interchangeably. Also in the present context, a word refers to a single FP64 number.","title":"Disclaimer"},{"location":"posts/post2/#hardware-trends","text":"Looking at Figure 1, memory transfers, within DRAM, or over the network, have been more expensive than (FP64) math operation since ~1992: Figure 1: Evolution of the time per flop (gamma), inverse bandwidth (beta) and latency (alpha) between ~1980 to ~2015. Source . The graph stops around 2015, where the ratio of gamma to beta (DRAM) was around 10. Let's look at the current FP64 Flop Per Load (FPL) factor for more recent hardware: GPU Release Year FP64 FLOPS (TFLOPS) BW (TB/s) FPL V100 2017 7.066 0.897 ~65.19 A100 2020 9.746 1.56 ~49.9 H100 2022 25.61 2.04 ~ 100 B200 2024 62 8.20 ~60 Table 1: Evolution of the BW, FP64 flops and FPL for recent Nvidia GPU models. FPL is computed as \\(\\frac{FP64 \\ Flops}{BW}*8\\) since a FP64 number is made of 8 bytes. Note It is illuminating to think about FPL as the average amount of FP64 operation you can do in the time that it takes to load one (non-cached) word from DRAM. As we can see, the FPL has been varying between 50 and 100. This is really large, and should motivate you to think really hard about how you access memory. In fact, communication avoiding algorithms have been a very active and fruitful research area which lead to the development of the BLAS standard, the Lapack library, and much more.","title":"Hardware trends"},{"location":"posts/post2/#some-naming-conventions","text":"I want to clarify some conventions, and push for the use of clear terminologies when talking about numerical algorithms. Let's take some time to realize that the following terms refer to very different things (inspired by Mark Hoemmen's PhD introduction ): An operation is a theoretical construction for data transformation e.g. matrix multiplication, finite-element projection, a time-step of a finite volume method, a linear system resolution. It exists in the realm of ideas, may have inserting properties but does not refer to specific way to achieve to the result. An algorithm: (for an operation) is a sequence of instructions, that may be written in pseudo-code, that describes a way how to get the result of an operation. Different algorithms may give different results (in exact arithmetic) for a given operation because of e.g. different convergence rate / error threshold. An implementation: (of an algorithm) is a concrete, existing piece of code that, well, implements an algorithm. It may be in done in any language. Different implementations should give the same results in exact arithmetic for a given algorithm, but can give different results in finite arithmetic because of e.g. floating point precision and hardware/runtime details.","title":"Some naming conventions"},{"location":"posts/post2/#a-simple-memory-model","text":"One result that I like a lot is the one presented in the second CS267 lecture where the following simple memory model is proposed: Assume a simple machine with just 2 levels of memory, fast and slow (think of e.g. DRAM / registers) and the following properties and notations: \\(M=\\) number of words that fits into fast memory, no latency (simplifying assumption), \\(t_m=\\) time per slow memory operation e.g. to move a word from fast to slow memory (inverse BW from Table 1 multiplied by 8 in our case since we are doing FP64 and ignoring latency), \\(t_f=\\) time per arithmetic operation i.e. the inverse of the FLOPS in Table 1. Assume an implementation of an algorithm, for which the runtime on the machine is characterized by: \\(m=\\) number of words moved between fast and slow memory to complete the algorithm, \\(f=\\) number of arithmetic operations to complete the algorithm, We can then define \\(CI_{\\text{runtime}}=\\frac{f}{m}\\) , a property of the runtime of an implementation of the algorithm that is called the computational intensity . It is the average number of flops per slow memory access. While the previously defined FPL factor, a property of the machine , is just given as \\(FPL_{\\text{hardware}}=\\frac{t_m}{t_f}\\) . Note: Nvidia GPUs have 4 levels of memory: DRAM, L2 and L1 caches, and registers. Each level has ~an order of magnitude difference in bandwidth. Some CPUs have 5 levels with an additional L3 cache. Real memory models are super complicated ! However, it is clear that memory hierarchies are omnipresent and that thinking hard about them helps both CPU and GPU performances.","title":"A simple memory model"},{"location":"posts/post2/#getting-good-performance","text":"The minimum possible time for our algorithm is \\(t_{\\text{ideal}}=ft_f\\) , which is attained when the problem fits in fast memory ( \\(m<M\\) ) and no slow memory transaction are required. This implies that we don't read any initial data from slow memory nor store in it, this is never the case in practice. Let's compare this to the real time for a big enough problem \\(t_{\\text{real}}=ft_f+mt_m\\) which rewrites: \\(t_{\\text{real}}= t_{\\text{ideal}}(1+\\frac{FPL_{\\text{hardware}}}{CI_{\\text{runtime}}})\\) It is now clear that to get near optimal performance, we want to reduce the ratio \\(\\frac{FPL_{\\text{hardware}}}{CI_{\\text{runtime}}}\\) as much as possible. Since \\({FPL_{\\text{hardware}}}\\) is a property of the hardware, with a value ranging between 50 and 100 depending on the GPU considered, all we can do is try to reduce \\(\\frac{1}{CI_{\\text{runtime}}}=\\frac{m}{f}\\) , by trying to reuse the data we load as much as possible.","title":"Getting good performance"},{"location":"posts/post2/#properties-of-the-runtime-vs-properties-of-an-implementation","text":"I insist on using the terminology \"properties of the runtime (of an implementation (of an algorithm)) \". Indeed, in practice, the numbers \\(f\\) , \\(m\\) and \\(CI_{\\text{runtime}}=\\frac{f}{m}\\) should not be obtained by simply computing the ratio of how much memory should be touched, and how many operation should be done ideally, optimally for a given operation. Because most real problems do not fit in cache. Instead, these numbers are a property of how the algorithm is implemented, compiled and ran. Performance for an operation can vary dramatically between a naive and a smart algorithm. Performance of an algorithm can vary dramatically between a naive and a smart implementation. Performance of an implementation can vary dramatically between a \"release\" build and a \"debug\" build, and between an \"old, crippled, small-cached\" machine and a \"brand new shiny and expensive\" machine. A big chunk of the implementation work is to force the compile-runtime pipeline to deliver the values of \\(f\\) and \\(m\\) that you desire. In this sense, I find CPU optimization is harder than GPU optimization because the gap between the implementation and the runtime is wider. In GPU programming, you are writing native SIMD code, and you can event control the L1 cache via shared memory. For CPU programming, you cannot control the cache, and using SIMD instructions is a pain. You have to write code and hope that the compiler/runtime does a good job of doing what you want it to do. Also, the Nvidia profilers are just fantastic. But this could be (and probably is) an exposition bias from me. Let's consider the example of a very generic operation: dense matrix multiplication , \\(C=A.B\\) (the 1st homework of CS267 and topic of the 2nd and 3rd lectures). Note matrix-multiplication is such a constrained operation that it is difficult to differentiate the operation from the algorithm. In fact, the difference between algorithms is the way they handle cache. If three \\(n\\times n\\) matrices fits in fast memory, we know that that we need to load/store only \\(3n^2\\) words (2 matrix read, 1 matrix write) from slow memory, and perform \\(2n^3\\) operations (one dot product per element of C, each dot product being \\(n\\) multiply and \\(n\\) add) with a resulting \\(CI_{\\text{ideal}}^{\\text{matmul}}=\\frac{3n}{2}\\) . The bigger \\(n\\) is, the closer we get from ideal performance. However, as \\(n\\) grows, it is clear that the problem does not fit in fast memory ( \\(3n^2>M\\) eventually). A naive implementation of matrix multiply is: for i in range(n): #load A[i,:] from fast memory (n words) for j in range (n): #load B[:,j] from fast memory (n words) #read C[i,j] from fast memory (1 word) C[i,j] = dot(A[i,:], B[:,j]) #(2n operations) #store C[i,j] in slow memory (1 word) #Total: # m = n^2 + n^3 + 2n^2 --> n^3, # f = 2n^3. It has a computational intensity of \\(CI_{\\text{naive}}^{\\text{matmul}}=\\mathcal{O}(2)\\) , which is terrible (compared to the ideal value) ! On the other hand, the well-known blocked implementation that splits and iterates over \\(b\\times b\\) sub-blocks of the matrices has a computational intensity of \\(CI_{\\text{blocked}}^{\\text{matmul}}=\\mathcal{O}(b)\\) , assuming that the blocks fit in fast memory: #pick a block size b that fits in cache (3b^2<M) N=n/b #compute block size for i in range(N): for j in range (N): #load the ijth block from C into fast memory (bxb words) Cblock=... for k in range (N) #load the ijth block from A into fast memory (bxb words) Ablock=... #load the ijth block from B into fast memory (bxb words) Bblock=... Cblock += matmul(Ablock, Bblock) #naive matmul, or micro kernel that fits in cache / registers (2b^3 operations) #store the ijth block from C into slow memory (bxb words) #Total: # m = N^2( 2b^2 + N(2b^2) ) --> 2N^3b^2= 2n^3/b # f = N^3(2b^3) =2n^3 So, you might wonder, what should I do ? How do I know if there is a better algorithm ? Well, a theoretical upper bound on the computational intensity has been found and is given by \\(CI_{\\text{blocked}}^{\\text{matmul}}=\\mathcal{O}(\\sqrt{M})\\) , and if you ever write a new dense matmul implementation, you should strive to reach it. And notice ! the blocked algorithm reaches that bound. Indeed, since the blocks fit in fast memory, \\(3b^2 <M\\) \\(\\implies\\) \\(b=\\mathcal{O}(\\sqrt{M})\\) . This is the whole point of communication avoiding algorithms research: computing lower bounds and finding algorithm that reaches them. Again, if you find this interesting, consider looking at this brilliant introduction .","title":"Properties of the runtime vs. properties of an implementation"},{"location":"posts/post2/#conclusion","text":"Well, all that is quite fascinating, but also overwhelming don't you think ? Well, you might not have to think about all this lower bound theory to get good speedups. In my first post on GPU kernel optimization I go over frequent coding mistakes that leads to extra useless communications. In that post, I will not give guidelines to reach theoretical lower bounds for your case. This is just too general to be discussed in a blog-post. As we saw, it constitutes a research topic on it's own and implies a deep re-thinking of the algorithms and data structures. No, here we will stay simple and focus on the following: given a GPU kernel, what frequent coding mistakes should we avoid to limit the amount of data we load/store from slow memory.","title":"Conclusion"},{"location":"posts/post3/","text":"","title":"(WIP) Testing mixed precision at runtime with verrou"},{"location":"posts/post4/","text":"","title":"(WIP) Scheduling different jobs on the same runner with Gitlab's CI"}]}
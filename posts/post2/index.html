<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>The cost of communications - Rémi Bourgeois's blog</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../css/timeago.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "The cost of communications";
        var mkdocs_page_input_path = "posts/post2.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script>
   
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4CKCXPSVEH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-4CKCXPSVEH');
  </script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Rémi Bourgeois's blog
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../about/">About</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Posts</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../post1/">(WIP) Five basic performance advices for porting kernels to the GPU</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">The cost of communications</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#disclaimer">Disclaimer</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#hardware-trends">Hardware trends</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#some-naming-conventions">Some naming conventions</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#a-simple-memory-model">A simple memory model</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#getting-good-performance">Getting good performance</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#properties-of-the-runtime-vs-properties-of-an-implementation">Properties of the runtime vs. properties of an implementation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#special-thanks">Special thanks</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../post3/">(WIP) Testing mixed precision at runtime with verrou</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../post4/">(WIP) Scheduling different jobs on the same runner with Gitlab's CI</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Rémi Bourgeois's blog</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Posts</li>
      <li class="breadcrumb-item active">The cost of communications</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="the-cost-of-communications">The cost of communications</h1>
<p><em>Last updated: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="September 8, 2025 15:02:29 UTC"><span class="timeago" datetime="2025-09-08T15:02:29+00:00" locale="en"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="September 8, 2025 15:02:29 UTC">2025-09-08</span></em>.<br />
<img alt="Visits" src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Frbourgeois33.github.io%2Fposts%2Fpost2%2F&amp;label=Visits" /></p>
<h2 id="disclaimer">Disclaimer</h2>
<p>This post was originally supposed to be a short introduction within <a href="../post1/">my first post on GPU kernel optimization</a>, but then I realized that I liked to talk too much about it and went out of scope. This is <strong>almost exclusively</strong> inspired by this brilliant talk: <a href="https://www.youtube.com/watch?v=iPCBCjgoAbk">Communication-Avoiding Algorithms for Linear Algebra, Machine Learning and Beyond</a> by Prof. James Demmel (UC Berkley), as well as a the <a href="https://sites.google.com/lbl.gov/cs267-spr2022">his 2022 CS267 class website</a> with free lectures on YouTube.</p>
<p><strong>Note</strong> The terms <em>communications</em> and <em>memory transfers</em> will be used interchangeably. Also in the present context, a <em>word</em> refers to a single FP64 number.</p>
<h2 id="hardware-trends">Hardware trends</h2>
<p>Looking at Figure 1, memory transfers, within DRAM, or over the network, have been more expensive than a (FP64 FMA) math operation since ~1992:
<img alt=" " src="../image.png" />
<strong>Figure 1:</strong> Evolution of the time per flop (gamma), inverse bandwidth (beta) and latency (alpha) between ~1980 to ~2015. <a href="https://extremecomputingtraining.anl.gov/wp-content/uploads/sites/96/2025/08/Communication-Avoiding-Algorithms-for-Linear-Algebra-Machine-Learning-and-Beyond-v2_ATPESC-2025.pdf">Source</a>.</p>
<p>The graph stops around 2015, where the ratio of gamma to beta (DRAM) was around 10. Let's look at the current FP64 <strong>Flop Per Load (FPL) factor</strong>, i.e. the average amount of FP64 operation you can do in the time that it takes to load one (non-cached) word from DRAM, for recent GPUs:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">GPU</th>
<th style="text-align: right;">Release Year</th>
<th style="text-align: right;">FP64 FLOPS (TFLOPS)</th>
<th style="text-align: right;">BW (TB/s)</th>
<th style="text-align: right;">FP64 FPL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957">Nvidia V100</a></td>
<td style="text-align: right;">2017</td>
<td style="text-align: right;">7.066</td>
<td style="text-align: right;">0.897</td>
<td style="text-align: right;">~65</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://www.techpowerup.com/gpu-specs/a100-pcie-40-gb.c3623">Nvidia A100</a></td>
<td style="text-align: right;">2020</td>
<td style="text-align: right;">9.746</td>
<td style="text-align: right;">1.56</td>
<td style="text-align: right;">~49</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://www.techpowerup.com/gpu-specs/radeon-instinct-mi250x.c3837">AMD MI250x</a></td>
<td style="text-align: right;">2021</td>
<td style="text-align: right;">47.85</td>
<td style="text-align: right;">3.28</td>
<td style="text-align: right;">~116</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://www.techpowerup.com/gpu-specs/h100-pcie-80-gb.c3899">Nvidia H100</a></td>
<td style="text-align: right;">2022</td>
<td style="text-align: right;">25.61</td>
<td style="text-align: right;">2.04</td>
<td style="text-align: right;">~ 100</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://www.techpowerup.com/gpu-specs/radeon-instinct-mi300a.c4296">AMD MI300A</a></td>
<td style="text-align: right;">2023</td>
<td style="text-align: right;">81.72</td>
<td style="text-align: right;">10.3</td>
<td style="text-align: right;">~63</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://www.techpowerup.com/gpu-specs/b200-sxm-192-gb.c4210">Nvidia B200</a></td>
<td style="text-align: right;">2024</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">8.20</td>
<td style="text-align: right;">~60</td>
</tr>
</tbody>
</table>
<p><strong>Table 1:</strong> Evolution of the BW, FP64 flops and FPL for recent GPUs. FPL is computed as <span class="arithmatex">\(\frac{FP64 \ Flops}{BW}*8\)</span> since a FP64 number is made of 8 bytes.</p>
<p><strong>Note:</strong> Modern GPUs such as the one in this table make up for the overwhelming majority of the computational capacity of the world's top supercomputers.</p>
<p>As we can see, the FPL has been varying between ~50 and 100. This is really large, and should motivate you to think really hard about how you access memory. In particular, you should avoid memory transfers, or communications as much as you possibly can. This is the idea behind <em>communication avoiding algorithms</em> research which lead to the development of the BLAS standard, the Lapack library, and much more.</p>
<h2 id="some-naming-conventions">Some naming conventions</h2>
<p>I want to clarify some conventions, and push for the use of clear terminologies when talking about numerical algorithms. Let's take some time to realize that the following terms refer to very different things (inspired by <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-37.pdf">Mark Hoemmen's PhD introduction</a>):</p>
<ul>
<li><strong>An operation</strong> is a theoretical construction for data transformation that takes an input, and produces an output, e.g. matrix multiplication, a stencil operation, a linear system resolution. It exists in the realm of ideas, may have interesting properties but does not refer to specific way to achieve to the result.</li>
<li><strong>An algorithm:</strong> (for an operation) is a sequence of instructions and so-called <em>kernels</em>, that may be written in pseudo-code, that describes a way how to get the result of an operation. Different algorithms may give different results for the same operation (even in exact arithmetic). For example, think about direct solvers vs. iterative solvers.</li>
<li><strong>An implementation:</strong> (of an algorithm) is a concrete, existing piece of code that, well, implements an algorithm. It may be in done in any language. Different implementations should give the same results in exact arithmetic for a given algorithm, but can give different results in finite arithmetic because of e.g. hardware/runtime details that affect the ordering of operations.</li>
</ul>
<p>When writing an algorithm, you always have to rely on building blocks, or <em>kernels</em>. The notion of <em>kernel</em> is relative to the level of abstraction you are working with. In particular:</p>
<ol>
<li>If you are designing a CFD solver, solving a linear system may be seen as a kernel.</li>
<li>If you are designing a linear system solver, performing a vector dot-product, or a matrix-vector multiplication may be seen as a kernel.</li>
<li>If you are designing a vector dot-product, loading data and performing multiplication and additions may be seen as a kernel.</li>
<li>If you are designing a floating-point addition, register manipulation may be seens as a kernel.</li>
</ol>
<p>The level of abstraction is reflected in the implementation which may rely on high-level libraries that already implements the kernels, so that you don't have to.</p>
<p><strong>Note:</strong> In the context of GPU programming such as my <a href="../post1/">my first post</a>, <em>kernel</em> often refer to the level 2. of abstraction, i.e. an algorithm for which the implementation fits in a single CUDA kernel, or equivalently within a <code>Kokkos::parallel_for</code>.</p>
<h2 id="a-simple-memory-model">A simple memory model</h2>
<p>One result that I like a lot is the one presented in <a href="https://www.youtube.com/watch?v=ictIJF2WXHE">the second CS267 lecture</a> where the following simple memory model is proposed:</p>
<ul>
<li>Assume a simple machine with just 2 levels of memory, fast and slow (think of e.g. DRAM / registers) and the following properties and notations:<ul>
<li><span class="arithmatex">\(M=\)</span> number of words that fits into fast memory,</li>
<li>no latency (simplifying assumption),</li>
<li><span class="arithmatex">\(t_m=\)</span> time per slow memory operation i.e. to move a word from fast to slow memory (inverse BW from Table 1 multiplied by 8 in our case since we are doing FP64 and ignoring latency),</li>
<li><span class="arithmatex">\(t_f=\)</span> time per arithmetic operation i.e. the inverse of the FLOPS in Table 1.</li>
</ul>
</li>
<li>Assume an implementation of an algorithm, for which the runtime on the machine is characterized by:<ul>
<li><span class="arithmatex">\(m=\)</span> number of words moved between fast and slow memory to complete the algorithm,</li>
<li><span class="arithmatex">\(f=\)</span> number of arithmetic operations to complete the algorithm.</li>
</ul>
</li>
</ul>
<p>We can then define <span class="arithmatex">\(CI_{\text{runtime}}=\frac{f}{m}\)</span>, a property  <strong>of the runtime of an implementation of the algorithm</strong> that is called <em>the computational intensity</em>. It is the average number of flops per slow memory access. While the previously defined FPL factor, <strong>a property of the machine</strong>, is given as <span class="arithmatex">\(FPL_{\text{hardware}}=\frac{t_m}{t_f}\)</span>.</p>
<p><strong>Note:</strong> Nvidia GPUs have 4 levels of memory: DRAM, L2 and L1 caches, and registers. Each level has ~3-4x difference in bandwidth. Some CPUs have 5 levels with an additional L3 cache. Real memory models are super complicated ! However, it is clear that memory hierarchies are omnipresent and that thinking hard about them helps both CPU and GPU performances.</p>
<h2 id="getting-good-performance">Getting good performance</h2>
<p>The minimum possible time for  our algorithm is <span class="arithmatex">\(t_{\text{ideal}}=ft_f\)</span>, which is attained when the problem fits in fast memory (<span class="arithmatex">\(m&lt;M\)</span>) and no slow memory transaction are required. This implies that we don't read any initial data from slow memory nor store in it, this is never the case in practice. </p>
<p>Let's compare this to the real time for a big enough problem <span class="arithmatex">\(t_{\text{real}}=ft_f+mt_m\)</span> which rewrites:</p>
<p><span class="arithmatex">\(t_{\text{real}}= t_{\text{ideal}}(1+\frac{FPL_{\text{hardware}}}{CI_{\text{runtime}}})\)</span></p>
<p>It is now clear that to get near optimal performance, we want to reduce the ratio <span class="arithmatex">\(\frac{FPL_{\text{hardware}}}{CI_{\text{runtime}}}\)</span> as much as possible. Since <span class="arithmatex">\({FPL_{\text{hardware}}}\)</span> is a property of the hardware, with a value ranging between 50 and 100 depending on the GPU considered, all we can do is try to reduce <span class="arithmatex">\(\frac{1}{CI_{\text{runtime}}}=\frac{m}{f}\)</span>, by trying to <strong>reuse</strong> the data we load as much as possible.</p>
<h2 id="properties-of-the-runtime-vs-properties-of-an-implementation">Properties of the runtime vs. properties of an implementation</h2>
<p>I insist on using the terminology <em>"properties of the </em><em>runtime (of an implementation (of an algorithm (for an operation)))</em><em>".</em> Indeed, in practice, the numbers <span class="arithmatex">\(f\)</span>, <span class="arithmatex">\(m\)</span> and <span class="arithmatex">\(CI_{\text{runtime}}=\frac{f}{m}\)</span> should not be obtained by simply computing the ratio of how much memory should be touched, and how many operation should be done ideally, optimally for a given operation. Because most real problems do not fit in cache. Instead, these numbers are a property of how the algorithm is implemented, compiled and ran;</p>
<ul>
<li>Operation count for an operation can vary dramatically between a naive and a smart algorithm. </li>
<li>Performance of an algorithm can vary dramatically between a naive and a smart implementation.</li>
<li>Performance of an implementation can vary dramatically between a "release" build and a "debug" build, and between machines of different quality.</li>
</ul>
<p>A big chunk of the implementation work is to force the compile-runtime pipeline to deliver the values of <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(m\)</span> that you desire. In this sense, I find CPU optimization is harder than GPU optimization because the gap between the implementation and the runtime is wider. In GPU programming, you are writing native SIMD code, and you can  control the L1 cache via <code>shared</code> memory. For CPU programming, you cannot control the cache, and using SIMD instructions is a pain. Also, the Nvidia profilers are just fantastic. But this could be (and probably is) an exposition bias from me. Most research papers talk about <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(m\)</span> as properties of the algorithm. This makes sense and is a useful approximation, but does implicitly assume that the algorithm is perfectly implemented. In reality, you have to write code and hope that the compiler/runtime does a good job of doing what you want it to do. </p>
<p>Let's consider the example of a very well-studied operation: the <span class="arithmatex">\(n\times n\)</span>, <strong>dense matrix multiplication</strong>, <code>C+=A.B</code> (1st homework of <a href="https://sites.google.com/lbl.gov/cs267-spr2022">CS267</a> and topic of the 2nd and 3rd lectures). </p>
<!--**Note** matrix-multiplication is such a constrained operation that it is difficult to differentiate the operation from the algorithm. In fact, the difference between algorithms is the way they handle cache.-->

<p>If three  matrices fits in fast memory, we know that that we need to load/store only <span class="arithmatex">\(4n^2\)</span> words (3 matrix read, 1 matrix write) from slow memory, and perform <span class="arithmatex">\(2n^3\)</span> operations (one dot product per element of C, each dot product being <span class="arithmatex">\(n\)</span> multiply and <span class="arithmatex">\(n\)</span> add) with a resulting <span class="arithmatex">\(CI_{\text{ideal}}^{\text{matmul}}=\frac{n}{2}\)</span>. The bigger <span class="arithmatex">\(n\)</span> is, the closer we get from ideal performance. However, as <span class="arithmatex">\(n\)</span> grows, it is clear that the problem will eventually not fit in fast memory (<span class="arithmatex">\(3n^2&gt;M\)</span>). A naive implementation of matrix multiply is:</p>
<pre><code class="language-python">for i in range(n):
    #load A[i,:] from fast memory (n words)
    for j in range (n): 
        #load B[:,j] from fast memory (n words)
        #read C[i,j] from fast memory (1 word)
        C[i,j] += dot(A[i,:], B[:,j]) #(2n+1 operations)
        #store C[i,j] in slow memory (1 word)

#Total: 
#  m = n^2 + n^3 + 2n^2 --&gt; n^3,
#  f = n^2 + 2n^3 --&gt; 2n^3.
</code></pre>
<p>It has a computational intensity of <span class="arithmatex">\(CI_{\text{naive}}^{\text{matmul}}=\mathcal{O}(2)\)</span>, which is terrible ! (compared to the ideal value). On the other hand, the well-known blocked implementation that splits and iterates over <span class="arithmatex">\(b\times b\)</span> sub-blocks of the matrices: </p>
<pre><code class="language-python">#pick a block size b that fits in cache (3b^2&lt;M)
N=n/b #compute block size

for i in range(N):
    for j in range (N):
        #load the ijth block from C into fast memory (bxb words)
        Cblock=...
        for k in range (N)
            #load the ijth block from A into fast memory (bxb words)
            Ablock=...
            #load the ijth block from B into fast memory (bxb words)
            Bblock=...
            Cblock += matmul(Ablock, Bblock) #naive matmul, or micro kernel that fits in cache / registers (2b^3 + 2b^2 operations)
        #store the ijth block from C into slow memory (bxb words)

#Total: 
# m = N^2( 2b^2 + N(2b^2) ) --&gt; 2N^3b^2= 2n^3/b
# f = N^3(2b^3+2b^2)  --&gt; 2N^3b^3 = 2n^3
</code></pre>
<p>has a computational intensity of  <span class="arithmatex">\(CI_{\text{blocked}}^{\text{matmul}}=\mathcal{O}(b)\)</span>, assuming that the blocks fit in fast memory.
So, you might wonder, what should I do ? How do I know if there is a better algorithm ? Well, a theoretical upper bound on the computational intensity has been found and is given by <span class="arithmatex">\(CI_{\text{blocked}}^{\text{matmul}}=\mathcal{O}(\sqrt{M})\)</span>, and if you ever write a new dense matmul implementation, you should strive to reach it. And notice ! the blocked algorithm reaches that bound. Indeed, since the blocks fit in fast memory, <span class="arithmatex">\(3b^2 &lt;M\)</span> <span class="arithmatex">\(\implies\)</span> <span class="arithmatex">\(b=\mathcal{O}(\sqrt{M})\)</span>. </p>
<p><strong>This is the whole point of <em>communication avoiding algorithms</em> research: computing lower bounds and finding algorithm that reaches them. Again, if you find this interesting, consider looking at this <a href="https://www.youtube.com/watch?v=iPCBCjgoAbk">brilliant introduction</a>.</strong></p>
<h2 id="conclusion">Conclusion</h2>
<p>Well, all that is quite fascinating, but also overwhelming don't you think ? Well, you might not have to think about all this lower bound theory to get good speedups. In <a href="../post1/">my first post on GPU kernel optimization</a> I go over frequent coding mistakes that leads to extra useless communications. In that post, I will not give guidelines to reach theoretical lower bounds for your case. This is just too general to be discussed in a blog-post. As we saw, it constitutes a research topic on it's own and implies a deep re-thinking of the algorithms and data structures. No, here we will stay simple and focus on the following: given a GPU kernel, what frequent coding mistakes should we avoid to limit the amount of data we load/store from slow memory.</p>
<h2 id="special-thanks">Special thanks</h2>
<p>Thanks to Ivan Huard (EPITA) for providing feedback.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../post1/" class="btn btn-neutral float-left" title="(WIP) Five basic performance advices for porting kernels to the GPU"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../post3/" class="btn btn-neutral float-right" title="(WIP) Testing mixed precision at runtime with verrou">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../post1/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../post3/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../js/timeago.min.js"></script>
      <script src="../../js/timeago_mkdocs_material.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>

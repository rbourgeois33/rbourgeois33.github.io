<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Six basic performance advices for porting kernels to the GPU - My Blog</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Six basic performance advices for porting kernels to the GPU";
        var mkdocs_page_input_path = "posts/post1.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script>
   
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4CKCXPSVEH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-4CKCXPSVEH');
  </script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> My Blog
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Posts</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Six basic performance advices for porting kernels to the GPU</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#0-introduction">0. Introduction</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#some-context-and-motivations">Some context and motivations</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#disclaimers">Disclaimers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pre-requisits">Pre-requisits</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#outline">Outline</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#before-we-start">Before we start</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#1-minimise-thread-level-redundant-global-memory-accesses">1. Minimise thread-level redundant global memory accesses</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#a-first-simple-example-temporary-register-storages">A first simple example, temporary register storages</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#profiler-diagnosis">Profiler diagnosis</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#static-arrays-as-temporary-storages">Static arrays as temporary storages</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#concluding-remarks-shared-memory-for-reducing-block-level-redundant-memory-accesses">Concluding remarks: shared memory for reducing block-level redundant memory accesses</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2-ensure-memory-access-are-coalesced">2. Ensure memory access are coalesced</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#background">Background</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#profiler-diagnosis_1">Profiler diagnosis</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advices">Advices</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#3-minimize-redundant-math-operation-use-cheap-arithmetics">3. Minimize redundant math operation, use cheap arithmetics</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#background_1">Background</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#profiler-diagnosis_2">Profiler diagnosis</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advices_1">Advices</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#4-understanding-occupancy">4. Understanding occupancy</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#background_2">Background</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#profiler-diagnosis_3">Profiler diagnosis</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advices_2">Advices</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#5-avoid-the-use-of-local-memory">5. Avoid the use of Local memory</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#background_3">Background</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#profiler-diagnosis_4">Profiler diagnosis</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advices_3">Advices</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#6-avoid-thread-divergence">6. Avoid thread divergence</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#background_4">Background</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#profiler-diagnosis_5">Profiler diagnosis</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advices_4">Advices</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#final-advices">Final advices</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../post2/">The cost of communications</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">My Blog</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Posts</li>
      <li class="breadcrumb-item active">Six basic performance advices for porting kernels to the GPU</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="six-basic-performance-advices-for-porting-kernels-to-the-gpu">Six basic performance advices for porting kernels to the GPU.</h1>
<h2 id="0-introduction">0. Introduction</h2>
<h3 id="some-context-and-motivations">Some context and motivations</h3>
<p>Hello world ! This is my first blog post. I'm RÃ©mi Bourgeois, PhD. I am a researcher engineer working at the French Atomic Energy Comission (CEA). I work on the <a href="https://cea-trust-platform.github.io/">TRUST platform</a>, a HPC (multi-GPU) CFD code that serves as a basis for many research/industrial applications.</p>
<p>I was hired by CEA to join the porting effort of this legacy code to the GPU using <a href="https://github.com/kokkos/kokkos">Kokkos</a>. This is quite a challenging task as the code is 20 years old, and more than 1400 kernels were identified to be ported to the GPU ! As I went and optimized some kernels, something struck me:</p>
<p><strong>The nature of the task of porting code to the GPU, especially when time is limited, often lead to small mistakes that can undermine performance.</strong></p>
<p>The goal of this blogpost is to give you <em>basic</em>, easy tips to keep in mind when writing / porting / first optimizing your kernels, so that you get a <em>reasonable</em> performance.</p>
<p>By applying them, I was able to get the following speedups that are measured relative to an already GPU-enabled baseline:</p>
<ul>
<li>A 40-50% speedup on a CFD <a href="https://github.com/cea-trust-platform/trust-code/blob/509d09ae94bc5189131c6f160f1d42f6024cfa98/src/VEF/Operateurs/Op_Conv/Op_Conv_VEF_Face.cpp#L473">convection kernel</a> from TRUST (obtained on RTX A5000, RTX A6000 Ada and H100 GPUs). <strong>Brace yourself</strong>: this is a monstruous kernel.</li>
<li>A 20-50% speedup on a CFD <a href="https://github.com/cea-trust-platform/trust-code/blob/509d09ae94bc5189131c6f160f1d42f6024cfa98/src/VEF/Operateurs/Op_Diff_Dift/Op_Dift_VEF_Face_Gen.tpp#L192">diffusion kernel</a> from TRUST (obtained on RTX A6000 Ada and H100 GPUs).</li>
<li>A 20% speedup on a <a href="https://github.com/Maison-de-la-Simulation/heraclespp/blob/54feb467f046cf21bdca5cfa679b453961ea8d7e/src/hydro/limited_linear_reconstruction.hpp#L54">MUSCL reconstruction kernel</a> from the radiative hydrodynamics code <a href="https://github.com/Maison-de-la-Simulation/heraclespp">heraclescpp</a> (obtained on a A100 GPU)</li>
<li>TODO: add ncu reports</li>
</ul>
<p>By <em>reasonable</em> I do not mean that you will get <em>optimal</em> performance. In fact, I will not go over what I consider to be <em>advanced</em> optimization advices such as the use of </p>
<ul>
<li><a href="https://www.youtube.com/watch?v=A1EkI5t_CJI&amp;t=5s"><code>shared memory</code></a>, </li>
<li><a href="https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/">vectorized operations</a>,</li>
<li><a href="https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/">tensor cores operations</a>,</li>
<li><a href="https://www.nvidia.com/en-us/on-demand/session/gtc25-s72683/?playlistId=playList-600dacf3-7db9-45fe-b0a2-e0156a792bc5">hardware-specific optimizations</a>. </li>
</ul>
<p>If getting  <em>optimal</em> performance is crucial to your application, consider learning more and apply these, but keep in mind that <strong>performance often comes at the cost of portability</strong>.
The advices are general enough so that they should allow speedups on all cards from all vendors.  By <em>advanced</em>, I do not mean that these topics are especially difficult or out of reach, but only that they require a significant design effort to be used effectively in a production context such as a wide CFD code like TRUST. In contrast, I believe that the advices I will give to you in this blogpost are easy enough so that you can, and should apply them straightforwardly while porting your code to the GPU in a time limited environement. </p>
<p><strong>Note 1:</strong> The target audience is engineer / researcher that want to get started with GPU porting in a code that relies on custom, domain specific low-level kernel. But do not reinvent the wheel ! i.e. do not rewrite kernels that have been implemented, hihgly optimized and distributed in libraries. Consider looking into (non exhaustive list !):</p>
<ul>
<li><a href="https://docs.nvidia.com/cuda-libraries/index.html">CUDA Libraries</a>.</li>
<li><a href="https://github.com/kokkos/kokkos-kernels">kokkos kernels</a> for portable BLAS, sparse BLAS and fraph kernels.</li>
<li><a href="https://trilinos.github.io/">Trilinos</a> for high level, portable solutions for the solution of large-scale, complex multi-physics engineering and scientific problems.</li>
<li><a href="https://petsc.org/release/">PETSc</a> for the scalable solution of scientific applications modeled by partial differential equations (PDEs)</li>
</ul>
<h3 id="disclaimers">Disclaimers</h3>
<p>If you think I wrote something that is wrong, or misleading please let me know !</p>
<p>I am running my performance tests on Nvidia GPUs, just because they are more easily available to me, and that I am more familiar with the performance tools such as <a href="https://developer.nvidia.com/nsight-systems">nsight systems</a> (nsys) and <a href="https://developer.nvidia.com/nsight-compute">nsight compute</a> (ncu). However, note that AMD provides similar profilers, and that the advices that I give here are simple enought so that they apply for GPUs from both vendors.</p>
<p>Moreover, I will use Kokkos as the programming model, just because I work with it, and that performance portability is <strong>cool</strong>. Again, the concepts are simple enought so that you can translate them to your favorite programming model, OpenMP, SYCL, Cuda, Hip.</p>
<h3 id="pre-requisits">Pre-requisits</h3>
<p>In this small tutorial, I will assume that you are already familiar with / will not cover:</p>
<ul>
<li>Basic C++.</li>
<li>The reason why you might want to use the GPU, and that you need a big enough problem to make full use of it.</li>
<li>How to Compile a GPU code, generate a report with <a href="https://youtu.be/04dJ-aePYpE?si=wTO9vJsRmVMBfM8a">Nvidia nsight compute</a> and loading in with the ui.</li>
<li>What is Kokkos, why you might want to use it and how to get started with it. Some ressources:<ul>
<li><a href="https://www.youtube.com/watch?v=y3HHBl4kV7g">Talk by Christian Trott, Co-leader of the Kokkos core team</a>.</li>
<li><a href="https://www.youtube.com/watch?v=rUIcWtFU5qM&amp;list=PLqtSvL1MDrdFgDYpITs7aQAH9vkrs6TOF">Kokkos lecture series</a> (kind of outdated, but you can find a lot of ressources online, alos, join the slack !).</li>
<li><strong>Note:</strong> you really <em>should</em> consider using Kokkos, or any other portable programming model. It's good enough so that CEA adopted it for it's legacy codes ! (see <a href="https://cexa-project.org/">the CExA project</a>).</li>
</ul>
</li>
<li>Basic GPU architecture, in particular:<ul>
<li>That you should avoid host to device memory transfers.</li>
<li>The roofline performance model.</li>
<li>What does compute bound / memory bound mean.</li>
<li>Some knowledge about the memory hierarchy (registers, L1/L2 caches, DRAM) and the increasing cost of memory accesses.</li>
<li>What are CUDA threads / blocks, global and local memory spaces</li>
<li>Some ressources on GPU architecture / CUDA programming:<ul>
<li><a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62191/">1h30 lecture by Athena Elfarou</a></li>
<li><a href="https://www.youtube.com/watch?v=OsK8YFHTtNs&amp;list=PL6RdenZrxrw-zNX7uuGppWETdxt_JxdMj">13 lectures by Bob Crovella</a></li>
</ul>
</li>
</ul>
</li>
<li>Application-level optimization:<ul>
<li>How to build a sensible optimization roadmap with e.g. Nvidia Nsight System</li>
<li>How to ensure that it is worth it to optimize the kernel you are looking (Don't assume bottleneck, profile, assess, optimize).</li>
<li>Some ressouces:<ul>
<li><a href="https://www.youtube.com/watch?v=nhTjq0P9uc8&amp;list=PL6RdenZrxrw-zNX7uuGppWETdxt_JxdMj&amp;index=8">8th lecture from the Bob Crovella lecture series</a> which focuses on that topic.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="outline">Outline</h3>
<p>The outline for this post is the following 6 rules of thumbs,or advices, largely inspired by <a href="https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html">the Nvidia Ampere tuning guide</a>:</p>
<ol>
<li>Minimize thread-level redundant global memory accesses.</li>
<li>Ensure memory access are coalesced.</li>
<li>Minimize redundant math operation, use cheap arithmetics.</li>
<li>Understanding occupancy.</li>
<li>Avoid the use of <em>Local memory</em>.</li>
<li>Avoid thread divergence.</li>
</ol>
<p>Feel free to jump straight into your sections of interest. TODO: i want also to giuve insight on where to look in ncu</p>
<h3 id="before-we-start">Before we start</h3>
<p>Before going into the 6 advices, I invite you to read <a href="../post2/">my post on the cost of communications</a> that is a good, unesseray long introduction for advices 1. and 2. I also strongly advise watching <a href="https://www.youtube.com/watch?v=sY3bgirw--4">this brilliant talk on communication-avoiding algorithms</a>. Moreover, all sample code and ncu reports can be <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/tree/code-sample/code-sample">here</a> along with compilation and execution instructions. The reports were ran on my <a href="https://www.techpowerup.com/gpu-specs/rtx-6000-ada-generation.c3933">Nvidia RTX 6000 Ada generation</a> GPU.</p>
<h2 id="1-minimise-thread-level-redundant-global-memory-accesses">1. Minimise thread-level redundant global memory accesses</h2>
<p>As discussed in <a href="../post2/">my post on the cost of communications</a>, on recent GPUs (V/A/H/B100) it takes 50-100x more time to load a non cached FP64 double from global memory up to registers than computing a FMA math operation on that number. We call this ratio the <strong>flop per load</strong> (FPL). The cache hierachy does mitigates that number, but each and every access to global memory <em>is</em> more expensive than a register manipulation by a factor of at least 2-5x, assuming the variable is cached in L1. You should avoid them <em>at all cost</em>.</p>
<h3 id="a-first-simple-example-temporary-register-storages">A first simple example, temporary register storages</h3>
<p>Let's considers at <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-1.cpp">sample-1.cpp</a> where we create two Device views:
If you are not familiar with Kokkos views, in this context, they are just a container for a vector that resides on the GPU.</p>
<pre><code class="language-c++">const int size = 1&lt;&lt;27;
Kokkos::View&lt;float*&gt; A(&quot;A&quot;, size);
Kokkos::View&lt;float*&gt; B(&quot;B&quot;, size);
</code></pre>
<p>and perform the following, rather silly, kernel,</p>
<pre><code class="language-c++">Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 
    for (int k=0; k&lt;10; k++){
        A(i) += B(i-1);
        A(i) += B(i);
        A(i) += B(i+1);
    }
});
</code></pre>
<p>The issue is probably already striking to you: each instance of <code>A(i) +=</code> is a global memory R/W. The solution is straightforward and found in <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-1-fixed.cpp">sample-1-fixed.cpp</a>: use a temporary storage for <code>A</code>, and R/W only once per thread:</p>
<pre><code class="language-c++">Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 
    float tmp=0;
    for (int k=0; k&lt;10; k++){
        tmp += B(i-1);
        tmp += B(i);
        tmp += B(i+1);
    }
    A(i) += tmp;
});
</code></pre>
<p><strong>Note</strong>: Since each thread uses the same values of B as it's neighbors, <code>shared memory</code> could be used to further improve performance. However, this kernel is simple enough so that caches probably already do a enough good job.</p>
<p>With this simple change, we went from 60R, 30W per thread to 31R, 1W. You might think that it is such an obvious thing to avoid that it is not even worth talking about it. But I disagree ! Often, when first porting to kokkos, in a time limited environment, we simply replace the e.g. <code>std::vector</code> by <code>Kokkos::View</code> in the kernel body, check functionnality and move onto the next kernel, resulting in this issue hindering performance. Moreover, for more very long, intricate kernels with many Views, spotting and   removing redundant memory accesses is quite tedious. Try for e.g. <a href="https://github.com/cea-trust-platform/trust-code/blob/509d09ae94bc5189131c6f160f1d42f6024cfa98/src/VEF/Operateurs/Op_Conv/Op_Conv_VEF_Face.cpp#L473">this one</a>.</p>
<h3 id="profiler-diagnosis">Profiler diagnosis</h3>
<p>Let's look into the profiler report for the first sample code (Download it and load it in ncu ! Works even if you don't have a Nvidia GPU), <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-1.ncu-rep">sample-1.ncu-rep</a>. First, let's look at the GPU SOL section:
<img alt="alt text" src="../image-1.png" />
<strong>Figure 1:</strong> GPU SOL section for sample-1.cpp.</p>
<p>We can see that the memory is heavily used. This begs the question, are we using it effectively ? Let's go to the memory workload analysis and dissect some elements
<img alt="alt text" src="../image-3.png" />
<strong>Figure 2:</strong> Memory workload analysis for sample-1.cpp.</p>
<ul>
<li>Memory Throughput [Gbyte/s] 308.75 <ul>
<li>--&gt; This is much lower than my GPU's bandwidth of 960.0,</li>
</ul>
</li>
<li>Communications between DRAM (Device Memory) and L2:<ul>
<li>1.07 GB reads, which corresponds to <span class="arithmatex">\(2^{27}(\text{size}) \times 2 (\text{A and B}) \times 4 (\text{bytes per double})=1.07\times 10^9\)</span> bytes. There is half as much writes, corresponding to A being modified. Both A and B are loaded once into the L2 cache, and A is written back only once into DRAM Good !</li>
</ul>
</li>
<li>Communications between L2 and L1:<ul>
<li>About as much reads into L1, a little more probably due to cache misses. But, an astounding 9.14 GB of data written from L1 to L2, due to cache invalidations ! This  is a great hint of redundant memory accesses; a big discrepancy between expected and observed volumes of exchanges. Essentially, this is the cache that is working hard to save you from you own mistakes, by not writing back all the way to DRAM at each <code>A(i)+=</code>. It really is saving you, as if we switch to throuput view, we see that these excessinve writes are done at an astounding 1.89 TB/s, twice as fast as my GPU's bandwith !</li>
</ul>
</li>
</ul>
<p>Lastly, let's look at one of my favorite sections, the warp state statistics session, espacially the warp states:
<img alt="alt text" src="../image-7.png" />
<strong>Figure 3:</strong> Warp States for sample-1.cpp.
If you are not familiar with warps states, really consider looking at <a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62191/">the 1h30 lecture by Athena Elfarou</a>. Essentially, a warp is a group of 32 threads (32 instances of the kernel with neigbouring index <code>i</code>). It can be either:</p>
<ul>
<li>Stalled: waiting on a dependancy: for instance, to perform <code>A(i)+=B(i)</code>, you need to wait that both values were loaded from global memory into the registers. This takes a while. You can also be waiting on a math operation to be done, a barrier, or the execution queue to have a spot.</li>
<li>Eligible: available to be scheduled,</li>
<li>Selected: will issue an instruction on the next cycle.</li>
</ul>
<p>The warp states shows you the <strong>reasons</strong> why your warps have been stalled during the execution of your kernel, sorted by importance. This is precisely what you should worry about ! We see two reasons here with quite obscure names "Stall long scoreboard and Stall LG throttle". You can drag over your mouse onto the items to get an explanation, e.g. for the first one:
<img alt="alt text" src="../image-6.png" />
<strong>Figure 4:</strong> Metric information for Stall long scoreboard.</p>
<p>Stall long scoreboard means that warps are waiting on a memory dependancy from global memory, this not surprising and a very common one for memory bound kernels. Stall LG throttle means that the warps are waiting on the warp slot queue to have a spot to be scheduled. Indeed, each warp scheduler has a finite amount of spots for it's warps to be scheduled. If a kernel issues too many requests, warps are waiting, not on a dependancy, but simply on a spot in the queue. This is also a good symptom of redundant memory operations !</p>
<p>Let't now take a look at <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-1-fixed.ncu-rep">sample-1-fixed.ncu-rep</a>. I recommend using the "add baseline" functionnality, so that we can track our progress ! First thing you can notice is that we get a huge performance gain: from 5.08ms to 1.81ms, a 64% speedup ! Then, going into the several sections:</p>
<ul>
<li>GPU Speed of light throuput:<ul>
<li>The compute pipeline is less busy, I'm honestly not sure why.</li>
<li>The memory pipeline is more used (+6%)</li>
</ul>
</li>
</ul>
<p><img alt="alt text" src="../image-8.png" />
<strong>Figure 5:</strong> Memory workload analysis for sample-1-fixed.cpp.</p>
<ul>
<li>Memory workload Analysis: <ul>
<li>The memory throuput is much closer to the theoretical peak (865 GB/s, +180%)</li>
<li>The previously healthy memory transfers are unchanged, but the L1 to L2 writes are reduced by 94%, as well as the caches hit rates. This shows that our cleaner implementation relies less on the caches, because it has much fewer redundant memory accesses.</li>
</ul>
</li>
</ul>
<!-- We can see that the kernel uses both memory and compute pipelines extensively. The high value of memory usage is surprising; Each thread is performing a lot of math; around 30 FMA, but is is much lower than the FP32 FPL of the GPU I am working with (A [Nvidia RTX 6000 Ada generation](https://www.techpowerup.com/gpu-specs/rtx-6000-ada-generation.c3933), with a FP32 FPL of 379) -->
<!-- - 255.85M request between the kernel and global memory, split amongst ~130M Reads and ~130M Writes.
    - This corresponds to $2^{27}(30) -->
<h3 id="static-arrays-as-temporary-storages">Static arrays as temporary storages</h3>
<p>Let's now consider a multi-dimensional case, with 2D Views and at least one run-time axis size, here, <code>dim</code>:</p>
<pre><code class="language-c++">const int size = 1&lt;&lt;27;
int dim = 3;
Kokkos::View&lt;float**&gt; A(&quot;A&quot;, size, dim);
Kokkos::View&lt;float**&gt; B(&quot;B&quot;, size, dim);
</code></pre>
<p>and the following kernel from <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-2.cpp">sample-2.cpp</a>:</p>
<pre><code class="language-c++">Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 
    for (int k = 0; k &lt; 10; k++){
        for (int dir = 0; dir &lt; dim; dir++){
            for (int dir2 = 0; dir2 &lt; dim; dir2++){
                A(i,dir) += B(i,dir2);
            }
        }
    }
});
</code></pre>
<p>It is clear that there are redundant memory accesses, that we would like to store in a array residing in registers as follows:</p>
<pre><code class="language-c++">Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 

    float Atmp[dim], Btmp[dim];

    for (int dir = 0; dir &lt; dim; dir++){
        Atmp[dir] = A(i, dir);
        Btmp[dir] = B(i, dir);
    }

    for (int k = 0; k &lt; 10; k++){
        for (int dir = 0; dir &lt; dim; dir++){
            for (int dir2 = 0; dir2 &lt; dim; dir2++){
            Atmp[dir] += Btmp[dir];
            }
        }
    }

    for (int dir = 0; dir &lt; dim; dir++){
        A(i,dir) = Atmp[dir];
    }
});
</code></pre>
<p>However, <code>dim</code> is a run-time variable, while arrays must be declared with compile time-value. In this case, this generate a compile error. If you somehow manage to get a run-time sized vector in your kernel and avoid a compilation error, you will at best get a slow runtime, as it will reside in local memory, cf. section 2. The solution is to wrap the kernel in a <code>dim</code>-templated function, as done in <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-2-fixed.cpp">sample-2-fixed.cpp</a>:</p>
<pre><code class="language-c++">template&lt;int dim&gt;
void apply_kernel(Kokkos::View&lt;float**&gt; A,  Kokkos::View&lt;float**&gt; B, int size){

    Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 
    // ... the previous kernel ...
    });
}
</code></pre>
<p>which can be called as follows, where dim is picked at runtime:</p>
<pre><code class="language-c++">if (dim==1){
    apply_kernel&lt;1&gt;(A, B, size);
} else if (dim==2){
    apply_kernel&lt;2&gt;(A, B, size);
} else if (dim==3){
    apply_kernel&lt;3&gt;(A, B, size);
} else{
   // Need more instantiations ! Fail or warning 
}
</code></pre>
<p>I will not go through the ncu reports for this second example as the behavior is really similar to sample-1, but feel free to look a them yourself. The speedup obtained is 75% which is not surpising, since I pick the examples.</p>
<h3 id="concluding-remarks-shared-memory-for-reducing-block-level-redundant-memory-accesses">Concluding remarks: shared memory for reducing block-level redundant memory accesses</h3>
<p>This section is limited to reducing <strong>thread-level</strong> redundant memory accesses. If you spot that nighbouring threads(that likely reside on the same block) are using extensively the same elements from global memory, I strongly suggest you learn more about <code>shared-memory</code>, to reduce redundant <strong>block-level</strong> memory accesses. <code>shared-memory</code> is essentially a portion of the L1 cache managed by the user. L1 cache is the fastest cache, right before registers, that is shared by threads of a block. But beware, using it means that you think you are smarter that the runtime ! :).</p>
<h2 id="2-ensure-memory-access-are-coalesced">2. Ensure memory access are coalesced</h2>
<h3 id="background">Background</h3>
<h3 id="profiler-diagnosis_1">Profiler diagnosis</h3>
<h3 id="advices">Advices</h3>
<p>Think about your data Layout Kokkos layout conspiracy The granularity of memory accesses: lost bytes</p>
<h2 id="3-minimize-redundant-math-operation-use-cheap-arithmetics">3. Minimize redundant math operation, use cheap arithmetics</h2>
<h3 id="background_1">Background</h3>
<h3 id="profiler-diagnosis_2">Profiler diagnosis</h3>
<h3 id="advices_1">Advices</h3>
<p>FMA, / vs *, unroll loop for int computation might need to template Math can be a bottleneck  Do smarter math</p>
<h2 id="4-understanding-occupancy">4. Understanding occupancy</h2>
<h3 id="background_2">Background</h3>
<h3 id="profiler-diagnosis_3">Profiler diagnosis</h3>
<h3 id="advices_2">Advices</h3>
<p>Hide latency The occupancy trap, ILP, hide latency reduce Register usage
Reduce usage, launch bound, no MDRANGE, we dont talk about block and share memory limits, template away expensive branches</p>
<h2 id="5-avoid-the-use-of-local-memory">5. Avoid the use of <em>Local memory</em></h2>
<h3 id="background_3">Background</h3>
<h3 id="profiler-diagnosis_4">Profiler diagnosis</h3>
<h3 id="advices_3">Advices</h3>
<p>Local memory is SLOW
Why does it spills + ref a la precedente section
How to avoid stack usage
attention aux tableaux statiques</p>
<h2 id="6-avoid-thread-divergence">6. Avoid thread divergence</h2>
<h3 id="background_4">Background</h3>
<h3 id="profiler-diagnosis_5">Profiler diagnosis</h3>
<h3 id="advices_4">Advices</h3>
<p>The SIMD pattern, masking templating</p>
<h2 id="final-advices">Final advices</h2>
<p>Participate to hackathons !</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../post2/" class="btn btn-neutral float-right" title="The cost of communications">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
      <span><a href="../post2/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>(WIP) Five basic performance advices for porting kernels to the GPU - Rémi Bourgeois's blog</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../css/timeago.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "(WIP) Five basic performance advices for porting kernels to the GPU";
        var mkdocs_page_input_path = "posts/post1.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script>
   
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4CKCXPSVEH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-4CKCXPSVEH');
  </script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Rémi Bourgeois's blog
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../about/">About</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Posts</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">(WIP) Five basic performance advices for porting kernels to the GPU</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#0-introduction">0. Introduction</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#some-context-and-motivations">Some context and motivations</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#disclaimers">Disclaimers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#prerequisites">prerequisites</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#outline">Outline</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#before-we-start">Before we start</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#detect-and-avoid-stack-usage">Detect and avoid stack usage</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#1-minimise-redundant-global-memory-accesses">1. Minimise redundant global memory accesses</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#1-minimize-redundant-thread-level-global-memory-accesses">1. Minimize redundant thread-level global memory accesses</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#a-first-simple-example-temporary-register-storage">A first simple example, temporary register storage</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#profiler-diagnosis">Profiler diagnosis</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#static-arrays-as-temporary-storages">Static arrays as temporary storages</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#minimize-block-level-redundant-memory-accesses-shared-memory">Minimize block-level redundant memory accesses: shared memory</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#minimize-redundant-kernel-level-memory-accesses-coalescing">Minimize redundant kernel-level memory accesses: coalescing</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#sectors-and-cache-line">sectors and cache line</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#profiler-diagnosis_1">Profiler diagnosis</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#advices">Advices</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2-avoid-the-use-of-local-memory">2. Avoid the use of local memory</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#what-is-local-memory-how-to-detect-its-usage">What is local memory, how to detect it's usage.</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#avoid-stack-usage">Avoid stack usage</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#precautions-when-using-static-arrays-for-temporary-storage">Precautions when using static arrays for temporary storage</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#profiler-diagnosis_2">Profiler diagnosis</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#avoid-register-spilling">Avoid register spilling</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#3-improve-occupancy">3. Improve occupancy</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#what-is-occupancy-why-improving-it">What is occupancy, why improving it</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#profiler-diagnosis_3">Profiler diagnosis</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#improve-occupancy">Improve occupancy</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#reduce-per-thread-register-usage">Reduce per-thread register usage</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#tune-launch-configuration">Tune launch configuration</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#reduce-shared-memory-usage">Reduce shared memory usage</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#4-minimize-redundant-math-operation-use-cheap-arithmetics">4. Minimize redundant math operation, use cheap arithmetics</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#background">Background</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#profiler-diagnosis_4">Profiler diagnosis</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advices_1">Advices</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#5-avoid-thread-divergence">5. Avoid thread divergence</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#background_1">Background</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#profiler-diagnosis_5">Profiler diagnosis</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advices_2">Advices</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#final-advices">Final advices</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../post2/">The cost of communications</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../post3/">(WIP) Testing mixed precision at runtime with verrou</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../post4/">(WIP) Scheduling different jobs on the same runner with Gitlab's CI</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Rémi Bourgeois's blog</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Posts</li>
      <li class="breadcrumb-item active">(WIP) Five basic performance advices for porting kernels to the GPU</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="wip-five-basic-performance-advices-for-porting-kernels-to-the-gpu">(WIP) five basic performance advices for porting kernels to the GPU.</h1>
<p><em>Last updated: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="September 5, 2025 07:32:48 UTC"><span class="timeago" datetime="2025-09-05T07:32:48+00:00" locale="en"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="September 5, 2025 07:32:48 UTC">2025-09-05</span></em>.<br />
<img alt="Visits" src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Frbourgeois33.github.io%2Fposts%2Fpost1%2F&amp;label=Visits" /></p>
<h2 id="0-introduction">0. Introduction</h2>
<h3 id="some-context-and-motivations">Some context and motivations</h3>
<p>I was hired by CEA to join the porting effort of this legacy code to the GPU using <a href="https://github.com/kokkos/kokkos">Kokkos</a>. This is quite a challenging task as the code is 20 years old, and more than 1400 kernels were identified to be ported to the GPU ! As I went and optimized some kernels, something struck me:</p>
<p><strong>The nature of the task of porting code to the GPU, especially when time is limited, often lead to small mistakes that can undermine performance.</strong></p>
<p>The goal of this blog-post is to give you <em>basic</em>, easy tips to keep in mind when writing / porting / first optimizing your kernels, so that you get a <em>reasonable</em> performance.</p>
<p>By applying them, I was able to get the following speedups that are measured relative to an already GPU-enabled baseline:</p>
<ul>
<li>A 40-50% speedup on a CFD <a href="https://github.com/cea-trust-platform/trust-code/blob/509d09ae94bc5189131c6f160f1d42f6024cfa98/src/VEF/Operateurs/Op_Conv/Op_Conv_VEF_Face.cpp#L473">convection kernel</a> from TRUST (obtained on RTX A5000, RTX A6000 Ada and H100 GPUs). <strong>Brace yourself</strong>: this is a monstruous kernel.</li>
<li>A 20-50% speedup on a CFD <a href="https://github.com/cea-trust-platform/trust-code/blob/509d09ae94bc5189131c6f160f1d42f6024cfa98/src/VEF/Operateurs/Op_Diff_Dift/Op_Dift_VEF_Face_Gen.tpp#L192">diffusion kernel</a> from TRUST (obtained on RTX A6000 Ada and H100 GPUs).</li>
<li>A 20% speedup on a <a href="https://github.com/Maison-de-la-Simulation/heraclespp/blob/54feb467f046cf21bdca5cfa679b453961ea8d7e/src/hydro/limited_linear_reconstruction.hpp#L54">MUSCL reconstruction kernel</a> from the radiative hydrodynamics code <a href="https://github.com/Maison-de-la-Simulation/heraclespp">heraclescpp</a> (obtained on a A100 GPU)</li>
</ul>
<p>[todo kernel are a single loop here]</p>
<p>By <em>reasonable</em> I do not mean that you will get <em>optimal</em> performance. In fact, I will not go over what I consider to be <em>advanced</em> optimization advices such as the use of </p>
<ul>
<li><a href="https://www.youtube.com/watch?v=A1EkI5t_CJI&amp;t=5s"><code>shared memory</code></a>, </li>
<li><a href="https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/">vectorized operations</a>,</li>
<li><a href="https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/">tensor cores operations</a>,</li>
<li><a href="https://www.nvidia.com/en-us/on-demand/session/gtc25-s72683/?playlistId=playList-600dacf3-7db9-45fe-b0a2-e0156a792bc5">hardware-specific optimizations</a>. </li>
</ul>
<p>If getting  <em>optimal</em> performance is crucial to your application, consider learning more and apply these, but keep in mind that <strong>performance often comes at the cost of portability</strong>.
The advices are general enough so that they should allow speedups on all cards from all vendors.  By <em>advanced</em>, I do not mean that these topics are especially difficult or out of reach, but only that they require a significant design effort to be used effectively in a production context such as a wide CFD code like TRUST. In contrast, I believe that the advices I will give to you in this blog post are easy enough so that you can, and should apply them straightforwardly while porting your code to the GPU in a time limited environment. </p>
<p><strong>Note 1:</strong> The target audience is engineer / researcher that want to get started with GPU porting in a code that relies on custom, domain specific low-level kernel. But do not reinvent the wheel ! i.e. do not rewrite kernels that have been implemented, highly optimized and distributed in libraries. Consider looking into (non exhaustive list !):</p>
<ul>
<li><a href="https://docs.nvidia.com/cuda-libraries/index.html">CUDA Libraries</a>.</li>
<li><a href="https://github.com/kokkos/kokkos-kernels">kokkos kernels</a> for portable BLAS, sparse BLAS and graph kernels.</li>
<li><a href="https://trilinos.github.io/">Trilinos</a> for high level, portable solutions for the solution of large-scale, complex multi-physics engineering and scientific problems.</li>
<li><a href="https://petsc.org/release/">PETSc</a> for the scalable solution of scientific applications modeled by partial differential equations (PDEs)</li>
</ul>
<h3 id="disclaimers">Disclaimers</h3>
<p>If you think I wrote something that is wrong, or misleading please let me know !</p>
<p>I am running my performance tests on Nvidia GPUs, just because they are more easily available to me, and that I am more familiar with the performance tools such as <a href="https://developer.nvidia.com/nsight-systems">nsight systems</a> (nsys) and <a href="https://developer.nvidia.com/nsight-compute">nsight compute</a> (ncu). However, note that AMD provides similar profilers (altough, at the time I am writing this, rocm's kernel profilers seem a lot less user friendly), and that the advices that I give here are general enough so that they apply for GPUs from both vendors.</p>
<p>I am heavily biased towards memory-related optimization as the CFD code that I am working on is heavily memory bound.</p>
<p>Moreover, I will use Kokkos as the programming model for the code sample, just because I work with it, and that performance portability is <strong>cool</strong>. Again, the concepts are simple enough so that you can translate them to your favorite programming model, OpenMP, SYCL, Cuda, Hip.</p>
<h3 id="prerequisites">prerequisites</h3>
<p>In this tutorial, I will assume that you are already familiar with:</p>
<ul>
<li>Basic C++.</li>
<li>The reason why you might want to use the GPU, and that you need a big enough problem to make full use of it.</li>
<li>Some understanding of GPU performance:<ul>
<li>The roof-line performance model.</li>
<li>What does compute bound / memory bound mean.</li>
</ul>
</li>
<li>Basic GPU architecture, in particular:<ul>
<li>Some knowledge about the memory hierarchy (registers, L1/L2 caches, DRAM) and the increasing cost of memory accesses.</li>
<li>What are CUDA threads / blocks, global memory. <em>You can be confused about what is local memory</em>.</li>
<li>Some knowledge of occupancy, altough not mandatory</li>
<li>Some resources on GPU architecture / CUDA programming:<ul>
<li>The refresher below,</li>
<li><a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62191/">1h30 lecture by Athena Elfarou (Nvidia)</a>,</li>
<li><a href="https://www.youtube.com/watch?v=OsK8YFHTtNs&amp;list=PL6RdenZrxrw-zNX7uuGppWETdxt_JxdMj">13 lectures by Bob Crovella (Nvidia)</a>.</li>
<li><a href="https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/achievedoccupancy.html">Achieved occupancy</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Although not necessary for getting through this post, I recommend you learn about:</p>
<ul>
<li>How to compile a GPU code, generate a report with <a href="https://youtu.be/04dJ-aePYpE?si=wTO9vJsRmVMBfM8a">Nvidia nsight compute</a> and loading in with the ui.</li>
<li>Application-level optimization:<ul>
<li>How to build a sensible optimization road-map with e.g. Nvidia Nsight System</li>
<li>That you should avoid host to device memory transfers; This tutorial is centered on kernel-level optimization. We assume memory is already available on the GPU</li>
<li>How to ensure that it is worth it to optimize the kernel you are looking (Don't assume bottleneck, profile, assess, optimize).</li>
<li>Some resources:<ul>
<li><a href="https://www.youtube.com/watch?v=nhTjq0P9uc8&amp;list=PL6RdenZrxrw-zNX7uuGppWETdxt_JxdMj&amp;index=8">8th lecture from the Bob Crovella (Nvidia) lecture series</a> which focuses on that topic.</li>
</ul>
</li>
</ul>
</li>
<li>What is Kokkos, why you might want to use it and how to get started with it. Some resources:<ul>
<li><a href="https://www.youtube.com/watch?v=y3HHBl4kV7g">Talk</a> by Christian Trott, Co-leader of the Kokkos core team (Sandia National Lab).</li>
<li><a href="https://www.youtube.com/watch?v=rUIcWtFU5qM&amp;list=PLqtSvL1MDrdFgDYpITs7aQAH9vkrs6TOF">Kokkos lecture series</a> (kind of outdated, but you can find a lot of resources online, alos, join the slack !).</li>
<li><strong>Note:</strong> you really <em>should</em> consider using Kokkos, or any other portable programming model. It's good enough so that CEA adopted it for it's legacy codes ! (see <a href="https://cexa-project.org/">the CExA project</a>).</li>
</ul>
</li>
</ul>
<h3 id="outline">Outline</h3>
<p>The outline for this post is the following five rules of thumbs,or advices, largely inspired by <a href="https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html">the Nvidia Ampere tuning guide</a>:</p>
<ol>
<li>Minimize redundant global memory accesses.</li>
<li>Avoid the use of <em>Local memory</em>.</li>
<li>Improve occupancy.</li>
<li>Minimize redundant math operation, use cheap arithmetic.</li>
<li>Avoid thread divergence.</li>
</ol>
<p>Feel free to jump straight into your sections of interest. One of the main interest of this tutorial is to teach you <em>where</em> to look for information in ncu. Look for "Profiler diagnosis" sections.</p>
<h3 id="before-we-start">Before we start</h3>
<p>Before going into the five advices, I invite you to read <a href="../post2/">my post on the cost of communications</a> that is a good, unnecessary long introduction for advices 1. and 2. I also strongly advise watching <a href="https://www.youtube.com/watch?v=iPCBCjgoAbk">this brilliant talk on communication-avoiding algorithms</a>. </p>
<p>All sample code and ncu reports can be found <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/tree/code-sample/code-sample">here</a> along with compilation and execution instructions. The reports were ran on my <a href="https://www.techpowerup.com/gpu-specs/rtx-6000-ada-generation.c3933">Nvidia RTX 6000 Ada generation</a> GPU.</p>
<p>Lastly, here is a refresher on the separation of hardware and software concepts when programming Nvidia GPUs with CUDA:
<img alt="alt text" src="../image-14.png" />
<strong>Figure 8:</strong> Software / hardware memory hierarchy of an Nvidia GPU. <a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62191/">source</a>. </p>
<p>The GPU hardware is organized as follows:</p>
<ul>
<li>A GPU is made of an aggregation of Streaming Multiprocessors (SM) that contains the computational units. This is where the computations are done. Instructions are scheduled as so-called warps, i.e. intruction packs of size 32.</li>
<li>The DRAM (slowest memory access, limited by the bandwidth), accessible by all Streaming Multiprocessors (SM),</li>
<li>the L2 cache, much smaller than the DRAM but faster, accessible by all SMs,</li>
<li>One L1 cache by SM, much smaller than the L2 but faster, accessible by only his host SM,</li>
<li>A register file per SM, much smaller than the L1 but the fastest, accessible by only his host SM.</li>
</ul>
<p>The software is organized as follows:</p>
<ul>
<li>Threads are uniquely defined sequences of operations defined by the CUDA kernels. They are dispatched on the GPU's SM.</li>
<li>Blocks of threads. Threads of a block only reside in the same SM.</li>
<li>Global memory. Visible by all threads, may reside in DRAM, L2 or L1 (potentially very slow !)</li>
<li>Shared memory. Visible by all threads of a block, managed by the developer. Resides in L1.</li>
<li><strong>Local memory.</strong> Private to a thread, may reside in DRAM, L2 or L1 (potentially very slow !). Local memory includes both register spilling and stack usage. More on that in section 2.</li>
<li>Registers. Private to a thread, resides in the SM's register file (the fastest !). </li>
</ul>
<h3 id="detect-and-avoid-stack-usage">Detect and avoid stack usage</h3>
<h2 id="1-minimise-redundant-global-memory-accesses">1. Minimise redundant global memory accesses</h2>
<p>On an Nvidia GPU, any request to global memory may end up fetching data from:</p>
<ul>
<li>The DRAM, visible by all threads</li>
<li>The L2 cache, visible by all threads</li>
<li>The L1 cache, visible by all threads of a block</li>
</ul>
<p>As discussed in <a href="../post2/">my post on the cost of communications</a>, on recent GPUs (V/A/H/B100) it takes 50-100x more time to load a non cached FP64 double from DRAM up to registers than computing a FMA math operation on that number. I call this ratio the <strong>flop per load</strong> (FPL). The cache hierarchy does mitigates that number. If the L1 cache hits, the memory request will be much faster, with a FPL of "only" 2-5x. For the L2 cache, the factor is about 5-10x. But it remains that every access to global memory <em>is</em> more expensive than a register manipulation by a factor of at least 2-5x. Thus you should avoid them <em>at all cost</em>.</p>
<h3 id="1-minimize-redundant-thread-level-global-memory-accesses">1. Minimize redundant thread-level global memory accesses</h3>
<h4 id="a-first-simple-example-temporary-register-storage">A first simple example, temporary register storage</h4>
<p>Let's considers at <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-1.cpp">sample-1.cpp</a> where we create two Device views:
If you are not familiar with Kokkos views, in this context, they are just containers for vectors that resides on the GPU.</p>
<pre><code class="language-c++">const int size = 1&lt;&lt;27;
Kokkos::View&lt;float*&gt; A(&quot;A&quot;, size);
Kokkos::View&lt;float*&gt; B(&quot;B&quot;, size);
</code></pre>
<p>and perform the following, rather silly, kernel,</p>
<pre><code class="language-c++">Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 
    for (int k=0; k&lt;10; k++){
        A(i) += B(i-1);
        A(i) += B(i);
        A(i) += B(i+1);
    }
});
</code></pre>
<p>The issue is probably already striking to you: each instance of <code>A(i) +=</code> is a global memory R/W. The solution is straightforward and found in <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-1-fixed.cpp">sample-1-fixed.cpp</a>: use a temporary storage for <code>A</code>, and R/W only once per thread:</p>
<pre><code class="language-c++">Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 
    float tmp=0;
    for (int k=0; k&lt;10; k++){
        tmp += B(i-1);
        tmp += B(i);
        tmp += B(i+1);
    }
    A(i) += tmp;
});
</code></pre>
<p><strong>Note</strong>: Since each thread uses the same values of B as it's neighbors, <code>shared memory</code> could be used to further improve performance. However, this kernel is simple enough so that caches probably already do a enough good job.</p>
<p>With this simple change, we went from 60R, 30W per thread to 31R, 1W. You might think that it is such an obvious thing to avoid that it is not even worth talking about it. But I disagree ! Often, when first porting to Kokkos, in a time limited environment, we simply replace the e.g. <code>std::vector</code> by <code>Kokkos::View</code> in the kernel body, check functionality and move onto the next kernel, resulting in this issue hindering performance. Moreover, for more very long, intricate kernels with many Views, spotting and   removing redundant memory accesses is quite tedious. Try for e.g. <a href="https://github.com/cea-trust-platform/trust-code/blob/509d09ae94bc5189131c6f160f1d42f6024cfa98/src/VEF/Operateurs/Op_Conv/Op_Conv_VEF_Face.cpp#L473">this one</a>.</p>
<h4 id="profiler-diagnosis">Profiler diagnosis</h4>
<p>Let's look into the profiler report for the first sample code (Download it and load it in ncu ! Works even if you don't have a Nvidia GPU), <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-1.ncu-rep">sample-1.ncu-rep</a>. First, let's look at the GPU SOL section:
<img alt="alt text" src="../image-1.png" />
<strong>Figure 1:</strong> GPU SOL section for sample-1.cpp.</p>
<p>We can see that the memory is heavily used. This begs the question, are we using it effectively ? Let's go to the memory workload analysis and dissect some elements
<img alt="alt text" src="../image-3.png" />
<strong>Figure 2:</strong> Memory workload analysis for sample-1.cpp.</p>
<ul>
<li>Memory Throughput [Gbyte/s] 308.75 <ul>
<li>--&gt; This is much lower than my GPU's bandwidth of 960.0,</li>
</ul>
</li>
<li>Communications between DRAM (Device Memory) and L2:<ul>
<li>1.07 GB reads, which corresponds to <span class="arithmatex">\(2^{27}(\text{size}) \times 2 (\text{A and B}) \times 4 (\text{bytes per double})=1.07\times 10^9\)</span> bytes. There is half as much writes, corresponding to A being modified. Both A and B are loaded once into the L2 cache, and A is written back only once into DRAM Good !</li>
</ul>
</li>
<li>Communications between L2 and L1:<ul>
<li>About as much reads into L1, a little more probably due to cache misses. But, an astounding 9.14 GB of data written from L1 to L2, due to cache invalidation ! This  is a great hint of redundant memory accesses; a big discrepancy between expected and observed volumes of exchanges. Essentially, this is the cache that is working hard to save you from you own mistakes, by not writing back all the way to DRAM at each <code>A(i)+=</code>. It really is saving you, as if we switch to throughput view, we see that these excessive writes are done at an astounding 1.89 TB/s, twice as fast as my GPU's bandwidth !</li>
</ul>
</li>
</ul>
<p>Lastly, let's look at one of my favorite sections, the warp state statistics session, especially the warp states:
<img alt="alt text" src="../image-7.png" />
<strong>Figure 3:</strong> Warp States for sample-1.cpp.
If you are not familiar with warps states, really consider looking at <a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62191/">the 1h30 lecture by Athena Elfarou (Nvidia)</a>. Essentially, a warp is a group of 32 threads (32 instances of the kernel with neighboring index <code>i</code>). It can be either:</p>
<ul>
<li>Stalled: waiting on a dependency: for instance, to perform <code>A(i)+=B(i)</code>, you need to wait that both values were loaded from global memory into the registers. This takes a while. You can also be waiting on a math operation to be done, a barrier, or the execution queue to have a spot.</li>
<li>Eligible: available to be scheduled,</li>
<li>Selected: will issue an instruction on the next cycle.</li>
</ul>
<p>The warp states shows you the <strong>reasons</strong> why your warps have been stalled during the execution of your kernel, sorted by importance. This is precisely what you should worry about ! We see two reasons here with quite obscure names "Stall long scoreboard and Stall LG throttle". You can drag over your mouse onto the items to get an explanation, e.g. for the first one:
<img alt="alt text" src="../image-6%20%281%29.jpg" /></p>
<p><strong>Figure 4:</strong> Metric information for Stall long scoreboard.</p>
<p>Stall long scoreboard means that warps are waiting on a memory dependency from global memory, this not surprising and a very common one for memory bound kernels. Stall LG throttle means that the warps are waiting on the warp slot queue to have a spot to be scheduled. Indeed, each warp scheduler has a finite amount of spots for it's warps to be scheduled. If a kernel issues too many requests, warps are waiting, not on a dependency, but simply on a spot in the queue. This is also a good symptom of redundant memory operations !</p>
<p>Let's now take a look at <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-1-fixed.ncu-rep">sample-1-fixed.ncu-rep</a>. I recommend using the "add baseline" functionality, so that we can track our progress ! First thing you can notice is that we get a huge performance gain: from 5.08ms to 1.81ms, a 64% speedup ! Then, going into the several sections:</p>
<ul>
<li>GPU Speed of light throughput:<ul>
<li>The compute pipeline is less busy, I'm honestly not sure why.</li>
<li>The memory pipeline is more used (+6%)</li>
</ul>
</li>
</ul>
<p><img alt="alt text" src="../image-8.png" />
<strong>Figure 5:</strong> Memory workload analysis for sample-1-fixed.cpp.</p>
<ul>
<li>Memory workload Analysis: <ul>
<li>The memory throughput is much closer to the theoretical peak (865 GB/s, +180%)</li>
<li>The previously healthy memory transfers are unchanged, but the L1 to L2 writes are reduced by 94%, as well as the caches hit rates. This shows that our cleaner implementation relies less on the caches, because it has much fewer redundant memory accesses.</li>
</ul>
</li>
</ul>
<!-- We can see that the kernel uses both memory and compute pipelines extensively. The high value of memory usage is surprising; Each thread is performing a lot of math; around 30 FMA, but is is much lower than the FP32 FPL of the GPU I am working with (A [Nvidia RTX 6000 Ada generation](https://www.techpowerup.com/gpu-specs/rtx-6000-ada-generation.c3933), with a FP32 FPL of 379) -->
<!-- - 255.85M request between the kernel and global memory, split amongst ~130M Reads and ~130M Writes.
    - This corresponds to $2^{27}(30) -->
<h4 id="static-arrays-as-temporary-storages">Static arrays as temporary storages</h4>
<p>Let's now consider a multi-dimensional case, with 2D Views and at least one run-time axis size, here, <code>dim</code>:</p>
<pre><code class="language-c++">const int size = 1&lt;&lt;27;
int dim = 3;
Kokkos::View&lt;float**&gt; A(&quot;A&quot;, size, dim);
Kokkos::View&lt;float**&gt; B(&quot;B&quot;, size, dim);
</code></pre>
<p>and the following kernel from <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-2.cpp">sample-2.cpp</a>:</p>
<pre><code class="language-c++">Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 
    for (int k = 0; k &lt; 10; k++){
        for (int dir = 0; dir &lt; dim; dir++){
            for (int dir2 = 0; dir2 &lt; dim; dir2++){
                A(i,dir) += B(i,dir2);
            }
        }
    }
});
</code></pre>
<p>It is clear that there are redundant memory accesses, that we would like to store in a array residing in registers as follows:</p>
<pre><code class="language-c++">Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 

    float Atmp[dim], Btmp[dim];

    for (int dir = 0; dir &lt; dim; dir++){
        Atmp[dir] = A(i, dir);
        Btmp[dir] = B(i, dir);
    }

    for (int k = 0; k &lt; 10; k++){
        for (int dir = 0; dir &lt; dim; dir++){
            for (int dir2 = 0; dir2 &lt; dim; dir2++){
            Atmp[dir] += Btmp[dir2];
            }
        }
    }

    for (int dir = 0; dir &lt; dim; dir++){
        A(i,dir) = Atmp[dir];
    }
});
</code></pre>
<p>However, <code>dim</code> is a run-time variable, while arrays must be declared with compile time-value. In this case, this generate a compile error. If you somehow manage to get a run-time sized vector in your kernel and avoid a compilation error, you will at best get a slow runtime, as it will reside in local memory, cf. section 2. The solution is to wrap the kernel in a <code>dim</code>-templated function, as done in <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-2-fixed.cpp">sample-2-fixed.cpp</a>:</p>
<pre><code class="language-c++">template&lt;int dim&gt;
void apply_kernel(Kokkos::View&lt;float**&gt; A,  Kokkos::View&lt;float**&gt; B, int size){

    Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 
    // ... the previous kernel ...
    });
}
</code></pre>
<p>which can be called as follows, where dim is picked at runtime:</p>
<pre><code class="language-c++">if (dim==1){
    apply_kernel&lt;1&gt;(A, B, size);
} else if (dim==2){
    apply_kernel&lt;2&gt;(A, B, size);
} else if (dim==3){
    apply_kernel&lt;3&gt;(A, B, size);
} else{
   // Need more instantiations ! Fail or warning 
}
</code></pre>
<p>I will not go through the ncu reports for this second example as the behavior is really similar to sample-1, but feel free to look a them yourself. The speedup obtained is 75% which is not surpising, since I pick the examples.</p>
<p><strong>Warning:</strong> Note that when using static arrays as temporary storage, you might accidentally trigger <em>Local memory</em> usage as it may resides in the stack. Please the section 2 to avoid this !</p>
<h3 id="minimize-block-level-redundant-memory-accesses-shared-memory">Minimize block-level redundant memory accesses: <code>shared memory</code></h3>
<p>If you spot that neighboring threads (that likely reside on the same block) are using extensively the same elements from global memory, I strongly suggest you learn more about <code>shared-memory</code>, to reduce redundant <strong>block-level</strong> memory accesses. <code>shared-memory</code> is essentially a portion of the L1 cache managed by the user. L1 cache is the fastest cache, right before registers, that is shared by threads of a block. But beware, using it means that you think you can do a better job than the runtime ! :). To use shared memory in Kokkos, look at <a href="https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/HierarchicalParallelism.html">Hierarchical Parallelism</a>. As mentionned in the introduction, I do not wish to delve deeper on this topic in this tutorial.</p>
<h3 id="minimize-redundant-kernel-level-memory-accesses-coalescing">Minimize redundant kernel-level memory accesses: coalescing</h3>
<h4 id="sectors-and-cache-line">sectors and cache line</h4>
<p>When using Nvidia GPUs, threads are packed in so-called warps of 32 threads which execute instructions simultaneously (SIMD). In the same way, when a memory request if performed, (like load one FP64 number from global memory), it is not done for a single one, but in packs of so-called <em>sectors</em> of 32 bytes (i.e. 4 FP64 numbers). Another way to say this is that the memory access granularity of the GPU is of 32 bytes. As a result, the best case scenario for a warp load, is that it requires data that is coalescing:</p>
<p><img alt="alt text" src="../image-9.png" />
<strong>Figure 6:</strong> Coalesced memory accesses <a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62191/">Source</a>. </p>
<p>In this ideal case, each thread is loading a FP64 number. There are 32 threads in a warps so this amount to 32 FP64 number, e.g. 256 bytes which is 8 sectors. This is very efficient because 256 bytes are loaded, and 256 bytes are used: the bandwidth of the GPU is fully used. Let's now look at the worst case scenario:</p>
<p><img alt="alt text" src="../image-10.png" />
<strong>Figure 7:</strong> strided memory accesses. Adapted from <a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62191/">source</a>. </p>
<p>In this case, each thread requires is also loading a FP64 numbers, but there is a sector-wide stride between threads. Since a FP64 number cannot be loaded "on it's own", a whole sector is loaded for each. As a result 32 sectors = 1024 bytes are loaded, for only 256 bytes used. This means that only a quarter of the bandwitdh is used, and the case gets worst if you are working with smaller a datatype.</p>
<h4 id="profiler-diagnosis_1">Profiler diagnosis</h4>
<p>The ratio of bytes loaded to bytes used per memory request is actually shown in the memory workload analysis. For example, for <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-1-fixed.ncu-rep">sample-1-fixed.ncu-rep</a>, we can see:
<img alt="alt text" src="../image-12.png" />
<strong>Figure 5:</strong> DRAM Global Load Access Pattern warning for <code>sample-1-fixed.cpp</code>.</p>
<p>where we see that for each 32 bytes sector transmitted, "only" 28.4 are used. This is pretty good of course as this code is very simple. But for more complicated operations, such as numerical simulation on unstructured meshes, this can be very bad. This section of ncu is helpful to detect that precise issue.</p>
<h4 id="advices">Advices</h4>
<p>These observations should make you want to think about your data layout. In particular, <strong>you should organize your data so that neighboring threads are working on neighboring data</strong>. This means that there is not one good general answer on which storage is better for a dense matrix, row-major or column-major. If you are doing matrix-vector product, sure, row-major is better as it will be more cache friendly. If you do matrix-transpose-vector product, you want column-major. In the same way, there is no definitive better answer to the debate Array of Structure vs. Structure of Array. It all depends on what you will be doing with the you data, how you will iterate over it. In other words, <strong>match your data layout with your iteration layout</strong>. You will hear people say things like <em>"LayoutLeft (column-major) is best on GPU, LayoutRight (row-major) is best on CPU"</em>. This is <strong>not</strong> a general truth.<!--- It might be the case for specific codes that work on arrays of specific size, for instance <code>A[M][N]</code> with <span class="arithmatex">\(M\gg N\)</span> and a specific type of operations on it. But this cannot be taught as a generality  ---> Let's look at an example:</p>
<p>Consider a matrix <code>A</code> of size, <code>NxN</code> and the following two kernels:</p>
<ol>
<li>compute <code>col</code>, a vector of <code>N</code> elements defined as the sum of each columns of <code>A</code>, by assigning each column to a thread,</li>
<li>compute <code>row</code>, a vector of <code>N</code> elements defined as the sum of each rows of <code>A</code>, by assigning each row to a thread.</li>
</ol>
<p><strong>Note:</strong> This is quite a bad way to parallelize this computation.</p>
<p>For kernel 1, storing <code>A</code> as column-major is better. Each thread is accessing a column which is a coalesced memory segment. If one uses row-major storage, we can expect terrible performance. The opposite argument applies for kernel 2. It will be fast for LayoutRight storage, and slow for LayoutLeft. In <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-3.cpp">sample-3.cpp</a>, we perform kernels 1 and 2 on a LayoutLeft Kokkos View:</p>
<pre><code class="language-c++">int N = 10000; 
Kokkos::View&lt;float**, Kokkos::LayoutLeft&gt;   A_LL(&quot;A_LL&quot;, N, N);      
Kokkos::View&lt;float**, Kokkos::LayoutRight&gt;  A_LR(&quot;A_LR&quot;, N, N);       
Kokkos::View&lt;float*&gt; row_sum(&quot;row_sum&quot;, N); 

// [...]

Kokkos::parallel_for(&quot;RowSumLL&quot;, N, KOKKOS_LAMBDA(int i) {
    float sum = 0.0;
    for (int j = 0; j &lt; N; j++) {
        sum += A_LL(i,j);
        }
    row_sum(i) = sum;
});

Kokkos::parallel_for(&quot;RowSumLR&quot;, N, KOKKOS_LAMBDA(int i) {
    float sum = 0.0;
    for (int j = 0; j &lt; N; j++) {
        sum += A_LR(i,j);
    }
    row_sum(i) = sum;
});
</code></pre>
<p>The report can be found at <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-3-fixed">sample-3.ncu-rep</a> in which we can see that:</p>
<ul>
<li><code>ColSumLL</code> runs in 0.52 ms,</li>
<li><code>RowSumLL</code> runs in 1.14ms (+120%), and the memory workload analysis section says: <em>"On average, only 4.0 of the 32 bytes transmitted per sector are utilized by each thread"</em>. Moreover, the L1 cache is used a lot, whereas <code>ColSumLL</code> does not use it at all.</li>
</ul>
<p>As you see, the best layout depends on what <em>operation</em> you plan on doing on your data, and <em>how</em> you plan on doing it. Another example is matrix multiplication where the best layout is cache/register-fitting blocked layout, <em>because</em> the best algorithm is cache/register-fitting blocked matrix multiplication. If you were to implement the naive version of matrix multiply <code>C=AB</code> where, for each element of <code>C</code>, you load a row of <code>A</code> and a column of <code>B</code> and perform the dot product, the best layout is storing <code>A/B</code> in row/column-major order respectively. For simulations on unstructured meshes, I recommend using <a href="https://en.wikipedia.org/wiki/Z-order_curve">Z-order curve</a> re-ordering of the mesh-element. For our CFD code TRUST, this enabled an overall +20% speedup due to a better coalescing.</p>
<p><strong>Note:</strong> Non-coalesced write are worst in performance that non-coalesced read, as a write needs to invalidate caches, we say that writes are "cache-through". A single non-coalesced write can invalidate a sector in L1, and L2, requiring the data to be fetched potentially all the way from DRAM for the next load.</p>
<p><strong>Note:</strong> The default layout for multidimensional views in Kokkos is LayoutLeft on device, and LayoutRight on host. I believe this is due to historical reasons; Algorithms from the Trilinos library that is built upon Kokkos runs more efficiently this way. But again, this is application-specific.</p>
<h2 id="2-avoid-the-use-of-local-memory">2. Avoid the use of local memory</h2>
<h3 id="what-is-local-memory-how-to-detect-its-usage">What is local memory, how to detect it's usage.</h3>
<p>As we saw in the introduction's refresher, local memory is private to a thread and may reside in DRAM, L2 or L1, which is potentially very slow, and in any case much slower than registers. Local memory usage happens in two cases:</p>
<ul>
<li>when the "stack" is used,</li>
<li>when register spilling happens.</li>
</ul>
<p>We will try to understand both and remove them from our kernels. One good thing about both of these is that the decision of using them is made at <em>compile-time</em> by nvcc. As a result, using the compile right flag (<code>-Xptas -v</code>), one can detect it easily, without having to run / profile the code. The flags are used in the sample code's <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/CMakeLists.txt">CMakeLists.txt</a>, and each kernel compilation produces such an output:</p>
<pre><code>ptxas info    : Compiling entry function '[verbose Kokkos mangled name]' for 'sm_89'
ptxas info    : Function properties for [verbose Kokkos mangled name]
    24 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 28 registers, used 0 barriers, 24 bytes cumulative stack size, 
                    512 bytes cmem[0]
ptxas info    : Compile time = 5.300 ms
</code></pre>
<p>if <code>bytes stack frame</code> is greater than 0, you are using the stack. If either <code>bytes spill stores</code> or <code>bytes spill loads</code>are greater than 0, register spilling is happening.</p>
<p><strong>Note:</strong> As far as I know there is not such a flag for AMD GPUs. You can however dump the <code>asm</code> and look for your kernel and the corresponding informations. This is possible, just less straighforward.</p>
<h3 id="avoid-stack-usage">Avoid stack usage</h3>
<h4 id="precautions-when-using-static-arrays-for-temporary-storage">Precautions when using static arrays for temporary storage</h4>
<p>In my experience, stack usage typically arises when using static arrays for temporary storage to reduce redundant thread-level global memory accesses. In particular, this happens when the array is accessed with a runtime variable, that the compiler cannot resolve at compile time. Because the compiler cannot predict the access pattern, it places the array in local memory, which provides the necessary flexibility for dynamic indexing despite terrible performances. Let's look at a mock example:</p>
<pre><code class="language-c++">// Within a GPU kernel, thread id is i
double tmp[3]
tmp[2] = ... //OK ! resolvable at compile time
int index = view_with_indexes_in_it(i) 
tmp[index] = .. //Not OK ! Compiler cannot resolve this. It cannot compute the value of index. tmp now resides in local memory
</code></pre>
<p>the solution is quite disappointing in my opinion, but necessary, if you are sure that values from <code>view_with_indexes_in_it(i)</code> are in <span class="arithmatex">\([0,2]\)</span>, then, you can apply a switch, i.e. replace the last line by:</p>
<pre><code class="language-c++">switch(index){
    case 0:
        tmp[0]=...; break;
    case 1:
        tmp[1]=...; break;    
    case 2:
        tmp[2]=...; break; 
    default:
        //Error 
}
</code></pre>
<p>This can also happens when static arrays are used in loops that cannot be unrolled, see <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-5.cpp">sample-5.cpp</a>:</p>
<pre><code class="language-c++">    int bound=3;
    Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 
        float tmp[3];  
        // some stuff
        for (int k=0; k&lt;bound; k++){ 
          tmp[k] += A(i+k);
        }
       // some stuff
    });
</code></pre>
<p>in this piece of code, <code>bound</code> is a non-const, runtime variable that nvcc cannot deduce when compiling the kernel. Therefore, <code>tmp</code> is not statically addressed and has to reside in local memory. Try compiling  as it is, and after adding a <code>const</code> clause in <code>bound</code>'s declaration, and observe the difference in the output pf <code>Xptas</code>.</p>
<h4 id="profiler-diagnosis_2">Profiler diagnosis</h4>
<p>Let's look at a <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-4.cpp">sample-4.cpp</a> that resembles sample-2, but with an indirection table that is filled</p>
<pre><code class="language-c++">Kokkos::View&lt;int**&gt; indirections(&quot;indirections&quot;, size, dim);
Kokkos::parallel_for(&quot;fill indirections&quot;, size, KOKKOS_LAMBDA(const int i) { 
    for (int dir = 0; dir &lt; dim; dir++){
        indirections(i, dir) = (dir+1)%dim; 
    }
});
</code></pre>
<p>such that it contains values in <span class="arithmatex">\([0, \text{dim}[\)</span>. Then, the table is used in the kernel:</p>
<pre><code class="language-c++">template&lt;int dim&gt;
void apply_kernel(Kokkos::View&lt;float**&gt; A,  Kokkos::View&lt;float**&gt; B, Kokkos::View&lt;int**&gt; indirections, int size){
Kokkos::parallel_for(&quot;Kernel&quot;, size, KOKKOS_LAMBDA(const int i) { 

        float Atmp[dim], Btmp[dim];
        int indir[dim];

        for (int dir = 0; dir &lt; dim; dir++){
            indir[dir] = indirections(i, dir); //Here we store the indirections
           // ... same as sample-2
        }
        for (int k = 0; k &lt; 10; k++){
            for (int dir = 0; dir &lt; dim; dir++){
                for (int dir2 = 0; dir2 &lt; dim; dir2++){
                    Atmp[indir[dir]] += Btmp[dir2]; //Not OK ! Compiler cannot resolve this. 
                    // It cannot predict the value of indir[dir]. 
                    // tmp now resides in local memory
                }
            }
        } 
        // ... same as sample-2
    });
}
</code></pre>
<p>The compilation produces the following compiler output:</p>
<pre><code>ptxas info    : Compiling entry function '[verbose Kokkos mangled name]' for 'sm_89'
ptxas info    : Function properties for [verbose Kokkos mangled name]
    16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 28 registers, used 0 barriers, 16 bytes cumulative stack size, 
                    512 bytes cmem[0]
ptxas info    : Compile time = 103.683 ms
</code></pre>
<p>To fix this, we use a switch in <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-4-fixed.cpp">sample-4-fixed.cpp</a>:</p>
<pre><code class="language-c++">switch (indir[dir]){
    case 0:
        Atmp[0] += Btmp[dir2];
        break;
    case 1:
        Atmp[1] += Btmp[dir2];
        break;
    case 2:
        Atmp[2] += Btmp[dir2];
        break;
}
</code></pre>
<p>which fixes the issue, as seen in the logs:</p>
<pre><code>    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 24 registers, used 0 barriers, 512 bytes cmem[0]
</code></pre>
<p>Now, comparing the ncu reports <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-4.ncu-rep">sample-4.ncu-rep</a>, <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-4-fixed.ncu-rep">sample-4-fixed.ncu-rep</a> we can see that:</p>
<ul>
<li>The fixed version is 32% faster (6.98ms vs 10.37 ms).</li>
<li>Both compute and memory pipelines are used much more effectively in the fixed version (see SOL section).</li>
<li>The warps are much less stalled for "stall long scoreboard" i.e. in this case waiting on local memory dependency for the fixed version.</li>
<li>The memory workload analysis allows to track local memory usage. For the fixed version, it's 0, -100% !
<img alt="alt text" src="../image-17.png" /></li>
</ul>
<p><strong>Figure 8</strong> Memory workload analysis for sample-4-fixed.ncu-rep, with sample-4.ncu-rep as a baseline. Zoom on Kernel &lt;-&gt; L1 interactions.</p>
<ul>
<li>We also see that the L1 cache is much less used (-92%) in the fixed version, because local memory resides in the L1 cache.</li>
<li>Lastly, the source view allows to track the line(s) responsible for local memory usage:
<img alt="alt text" src="../image-16.png" /><br />
<strong>Figure 9:</strong> Local memory usage localization in ncu's source view for <a href="https://github.com/rbourgeois33/rbourgeois33.github.io/blob/code-sample/code-sample/sample-4.ncu-rep">sample-4.ncu-rep</a>.</li>
</ul>
<p>As you can see, there are many ways to detect stack usage, at compile time with the right flags, or in ncu. This is also the case for register spilling, as detailed in the next section.</p>
<h3 id="avoid-register-spilling">Avoid register spilling</h3>
<p>Register spilling happens when threads are requiring too much registers, so much so that it hinders <em>occupancy</em> in such an extreme way that the compiler decides to "spill" the memory that should initially be in registers, into slow local memory. Therefore, advices on improving occupancy by reducing per-thread register usage will help avoiding register spilling. As a result, we refer to the next section as the advices will coincides</p>
<h2 id="3-improve-occupancy">3. Improve occupancy</h2>
<h3 id="what-is-occupancy-why-improving-it">What is occupancy, why improving it</h3>
<p>As we saw in the introduction's refresher, threads reside on the GPU's SM as warps of 32. Each SM can host a maximum given number of warps, e.g. 48 for my RTX 6000 Ada. If  a kernel is able to have 48 active warps per SM, we say that it runs at <em>maximum occupancy</em>.</p>
<p>However, several factor may hinder occupancy, indeed, each SM has a limited amount of:</p>
<ul>
<li>registers</li>
<li>number of blocks it can host</li>
<li>shared memory</li>
<li>threads it can host</li>
</ul>
<h3 id="profiler-diagnosis_3">Profiler diagnosis</h3>
<p>ncu latency bound, occupancy section
 The occupancy trap:ILP</p>
<h3 id="improve-occupancy">Improve occupancy</h3>
<h4 id="reduce-per-thread-register-usage">Reduce per-thread register usage</h4>
<p>no MDRANGE, template away expensive branches, reogranize math ? limited</p>
<h4 id="tune-launch-configuration">Tune launch configuration</h4>
<p>launch bound</p>
<h4 id="reduce-shared-memory-usage">Reduce shared memory usage</h4>
<p>we dont talk about block and share memory limits, </p>
<h2 id="4-minimize-redundant-math-operation-use-cheap-arithmetics">4. Minimize redundant math operation, use cheap arithmetics</h2>
<h3 id="background">Background</h3>
<h3 id="profiler-diagnosis_4">Profiler diagnosis</h3>
<h3 id="advices_1">Advices</h3>
<p>FMA, / vs *, unroll loop for int computation might need to template Math can be a bottleneck  Do smarter math</p>
<h2 id="5-avoid-thread-divergence">5. Avoid thread divergence</h2>
<h3 id="background_1">Background</h3>
<h3 id="profiler-diagnosis_5">Profiler diagnosis</h3>
<h3 id="advices_2">Advices</h3>
<p>The SIMD pattern, masking templating</p>
<h2 id="final-advices">Final advices</h2>
<p>Participate to hackathons !</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../.." class="btn btn-neutral float-left" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../post2/" class="btn btn-neutral float-right" title="The cost of communications">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../.." style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../post2/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../js/timeago.min.js"></script>
      <script src="../../js/timeago_mkdocs_material.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
